{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magedsaeed/.virtualenvs/generate-sequences/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from generate_sequences import GreedyGenerator, BeamSearchGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "model_name = \"gpt2\"  # You can choose other variants like 'gpt2-medium', 'gpt2-large', 'gpt2-xl'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token=tokenizer.decode(model.generation_config.bos_token_id)\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts to generate\n",
    "input_texts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The quick brown fox\",\n",
    "    \"Last night I dreamed\",\n",
    "    \"In the heart of the city\",\n",
    "    \"At the edge of the world\",\n",
    "]\n",
    "MAX_LENGTH = 50\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(texts, batch_size):\n",
    "    \"\"\"Yield successive n-sized batches from texts.\"\"\"\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        yield texts[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text using HuggingFace `generate` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8e36e8f0584a0d80ac5dedd1aa44c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Texts: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Once upon a time\n",
      "Generated: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n",
      "\n",
      "Input: The quick brown fox\n",
      "Generated: The quick brown foxes are a great way to get a little bit of a kick out of your dog.\n",
      "\n",
      "The quick brown foxes are a great way to get a little bit of a kick out of your dog. The quick brown fox\n",
      "\n",
      "Input: Last night I dreamed\n",
      "Generated: Last night I dreamed of a day when I could go to the beach and swim with my friends. I was so excited to be back in the ocean. I was so excited to be back in the ocean. I was so excited to be\n",
      "\n",
      "Input: In the heart of the city\n",
      "Generated: In the heart of the city, the city of San Francisco is a city of people. It's a place where people come together to celebrate, to celebrate, to celebrate. It's a place where people come together to celebrate, to celebrate, to\n",
      "\n",
      "Input: At the edge of the world\n",
      "Generated: At the edge of the world, the world is a place of great beauty. The world is a place of great fear. The world is a place of great fear. The world is a place of great fear. The world is a place of great\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_texts = []\n",
    "for batch in tqdm(get_batches(input_texts, BATCH_SIZE), desc=\"Generating Texts\"):\n",
    "    # Tokenize batch\n",
    "    encoded_input = tokenizer(\n",
    "        batch,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate text\n",
    "    output = model.generate(\n",
    "        input_ids=encoded_input[\"input_ids\"],\n",
    "        attention_mask=encoded_input[\"attention_mask\"],\n",
    "        max_length=MAX_LENGTH,  # Max length of the generated text\n",
    "    )\n",
    "\n",
    "    # Decode generated texts\n",
    "    batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n",
    "    generated_texts.extend(batch_generated_texts)\n",
    "\n",
    "# Print all collected results\n",
    "for input_text, generated_text in zip(input_texts, generated_texts):\n",
    "    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec64c9dafd24d9682e7b362a4612bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Texts: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Once upon a time\n",
      "Generated: Once upon a time, you can be a little bit of a celebrity. You can be a little bit of a celebrity. You can be a little bit of a celebrity. You can be a little bit of a celebrity. You can be a little\n",
      "\n",
      "Input: The quick brown fox\n",
      "Generated: The quick brown fox was quite happy and was beginning to think about something else.\n",
      "\n",
      "\"So what's the matter with you?\"\n",
      "\n",
      "\"Don't worry, I will be fine.\"\n",
      "\n",
      "\"What? You're not going to let\n",
      "\n",
      "Input: Last night I dreamed\n",
      "Generated: Last night I dreamed of getting my first ride on the subway. I was a little worried. I had never been on a subway before, but I was already doing it. It was a beautiful, beautiful day and I didn't know if\n",
      "\n",
      "Input: In the heart of the city\n",
      "Generated: In the heart of the city, the city's central business district has been transformed into a booming metropolis of shopping and restaurants.\n",
      "\n",
      "Now, more than 20 years after the city's revitalization, the city is in the midst of its biggest\n",
      "\n",
      "Input: At the edge of the world\n",
      "Generated: At the edge of the world, where the sun is shining in the west, the sun is shining in the east. The sun is shining in the east, and the sun is shining in the west. We are going to find out that the sun\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_texts = []\n",
    "for batch in tqdm(get_batches(input_texts, BATCH_SIZE), desc=\"Generating Texts\"):\n",
    "    # Tokenize batch\n",
    "    encoded_input = tokenizer(batch, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    output = model.generate(\n",
    "        input_ids=encoded_input[\"input_ids\"],\n",
    "        attention_mask=encoded_input[\"attention_mask\"],\n",
    "        max_length=MAX_LENGTH,  # Max length of the generated text\n",
    "        top_k=50,  # Limits the sampling pool to the top_k tokens\n",
    "        top_p=0.95,  # Nucleus sampling: sample only from top_p probability mass\n",
    "        temperature=0.7,  # Sampling temperature: lower value -> more conservative, higher value -> more random\n",
    "        do_sample=True  # Enable sampling\n",
    "    )\n",
    "    \n",
    "    # Decode generated texts\n",
    "    batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n",
    "    generated_texts.extend(batch_generated_texts)\n",
    "    \n",
    "# Print all collected results\n",
    "for input_text, generated_text in zip(input_texts, generated_texts):\n",
    "    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with generate-sequences, greedy generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_forward(encoder_inputs, decoder_inputs):\n",
    "    return model(input_ids=decoder_inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_greedy_generator = GreedyGenerator(\n",
    "    use_tqdm=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=MAX_LENGTH,\n",
    "    device=model.device,\n",
    "    generation_forward=generation_forward,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3047a10418246be9e678518da2ec439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c4e4c9c8ac41ea93b738eef7b50ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6488c26f3247a3bda10a06e546d997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Once upon a time\n",
      "Generated: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n",
      "\n",
      "Input: The quick brown fox\n",
      "Generated: The quick brown foxes are a great way to get a little bit of a kick out of your dog.\n",
      "\n",
      "The quick brown foxes are a great way to get a little bit of a kick out of your dog. The quick brown fox\n",
      "\n",
      "Input: Last night I dreamed\n",
      "Generated: Last night I dreamed of a day when I could go to the beach and swim with my friends. I was so excited to see the ocean, the waves, the waves. I was so excited to see the ocean, the waves, the\n",
      "\n",
      "Input: In the heart of the city\n",
      "Generated: In the heart of the city, the city of San Francisco is a city of people. It's a place where people come together to celebrate, to celebrate, to celebrate. It's a place where people come together to celebrate, to celebrate, to\n",
      "\n",
      "Input: At the edge of the world\n",
      "Generated: At the edge of the world, the world is a place of great beauty. The world is a place of great fear. The world is a place of great fear. The world is a place of great fear. The world is a place of great\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_texts = []\n",
    "for batch in get_batches(input_texts, BATCH_SIZE):\n",
    "    # Tokenize batch\n",
    "    encoded_input = tokenizer(batch, return_tensors=\"pt\", padding=True).to(device)\n",
    "    # Generate text\n",
    "    output = gpt2_greedy_generator.generate(\n",
    "        encoder_inputs=None,\n",
    "        decoder_inputs=encoded_input[\"input_ids\"],\n",
    "        pad_decoder_inputs=tokenizer.bos_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode generated texts\n",
    "    batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n",
    "    generated_texts.extend(batch_generated_texts)\n",
    "    \n",
    "# Print all collected results\n",
    "for input_text, generated_text in zip(input_texts, generated_texts):\n",
    "    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with generate-sequences, greedy with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_forward(encoder_inputs, decoder_inputs):\n",
    "    return model(input_ids=decoder_inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_greedy_generator = GreedyGenerator(\n",
    "    use_tqdm=True,\n",
    "    top_k_sampling=50,\n",
    "    top_p_sampling=0.95,\n",
    "    device=model.device,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=MAX_LENGTH,\n",
    "    multinomial_sampling=True,\n",
    "    generation_forward=generation_forward,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b60382cb5644117aaa1dfa69af69a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c521e704e8bf4a05b74ed9c41ac41b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db670ed5c8804b16b59f483a47b1438f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Once upon a time\n",
      "Generated: Once upon a time, I'd used all that 'love and friendship' I was left with. But this time around I'd had to start with the least powerful. That's when I started to realize how great it was… it didn't take\n",
      "\n",
      "Input: The quick brown fox\n",
      "Generated: The quick brown fox.\n",
      "\n",
      "-\n",
      "\n",
      "I'm still getting used to this outfit, but I have something I'm really craving.\n",
      "\n",
      "-\n",
      "\n",
      "What do you mean'something?'\n",
      "\n",
      "I see the two foxes now.\n",
      "\n",
      "Input: Last night I dreamed\n",
      "Generated: Last night I dreamed of meeting a guy I had met at a bar or concert. He was looking over my shoulder while I got on stage for a show. I asked him if he had ever had an alcohol problem, which he had!\n",
      "\n",
      "Input: In the heart of the city\n",
      "Generated: In the heart of the city. A small street. They want the children to learn to read, and get to the movies. You can't have that without a good neighborhood.\n",
      "\n",
      "We started out with a simple vision. We wanted to change\n",
      "\n",
      "Input: At the edge of the world\n",
      "Generated: At the edge of the world, they had to escape.\n",
      "\n",
      "Sylvester and I didn't. It was a dangerous situation when, during the summer, and also in the autumn, a large amount of people were attacked. One man\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_texts = []\n",
    "for batch in get_batches(input_texts, BATCH_SIZE):\n",
    "    # Tokenize batch\n",
    "    encoded_input = tokenizer(batch, return_tensors=\"pt\", padding=True).to(device)\n",
    "    # Generate text\n",
    "    output = gpt2_greedy_generator.generate(\n",
    "        encoder_inputs=None,\n",
    "        decoder_inputs=encoded_input[\"input_ids\"],\n",
    "        pad_decoder_inputs=tokenizer.bos_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode generated texts\n",
    "    batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n",
    "    generated_texts.extend(batch_generated_texts)\n",
    "    \n",
    "# Print all collected results\n",
    "for input_text, generated_text in zip(input_texts, generated_texts):\n",
    "    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate-sequences",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
