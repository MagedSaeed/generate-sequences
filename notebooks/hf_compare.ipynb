{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "import datasets\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "from generate_sequences import GreedyGenerator, BeamSearchGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the translation model from transformers\n",
    "# model_name = \"Helsinki-NLP/opus-mt-ar-en\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "model_name = \"marefa-nlp/marefa-mt-en-ar\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "bleu_scorer = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "test_dataset = datasets.load_dataset('iwslt2017','iwslt2017-ar-en', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = 'en'\n",
    "target_language = 'ar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " 10,\n",
       " [\"One major consequence of this work  is that maybe all of these decades,  we've had the whole concept of cybernetic revolt  in reverse.\",\n",
       "  \"It's not that machines first become intelligent  and then megalomaniacal  and try to take over the world.\",\n",
       "  \"It's quite the opposite,  that the urge to take control  of all possible futures  is a more fundamental principle  than that of intelligence,  that general intelligence may in fact emerge  directly from this sort of control-grabbing,  rather than vice versa.\",\n",
       "  'Another important consequence is goal seeking.',\n",
       "  \"I'm often asked, how does the ability to seek goals  follow from this sort of framework?\"],\n",
       " ['أحد العواقب الكبرى لهذا العمل هو أنه لربما طوال كل هذه العقود، كان لدينا المفهوم العكسي للثورة الآلية.',\n",
       "  'الأمر ليس في أن الآلات تصبح ذكية في البداية ثم ينتابها جنون العظمة و تحاول السيطرة على العالم.',\n",
       "  'إنه تماماً العكس، أن النزعة للسيطرة على كل الأزمنة المستقبلية الواردة هي مبدأ أساسي أكثر من مبدأ الذكاء، أن نواحي الذكاء العامة يمكن في الحقيقة أن تنبعث مباشرة من السيطرة، بدلاً من أن يكون الأمر بالعكس.',\n",
       "  'عاقبة أخرى مهمة هي البحث عن الهدف.',\n",
       "  'إنني أُسأل غالباً، كيف يمكن تفسير قدرة البحث عن الأهداف في هذا الإطار؟'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts = [example[source_language] for example in test_dataset['translation']][-10:]\n",
    "targets = [example[target_language] for example in test_dataset['translation']][-10:]\n",
    "len(input_texts), len(targets), input_texts[:5], targets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.num_beams=1\n",
    "model.generation_config.use_cache = False\n",
    "model.generation_config.batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(inputs,batch_size):\n",
    "    for i in tqdm(\n",
    "            range(0, len(inputs), batch_size),\n",
    "            desc=\"Generating Sequences\",\n",
    "            total=len(inputs) // batch_size,\n",
    "        ):\n",
    "        yield inputs[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "            # num_beams=4,\n",
    "            # do_sample=True,\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5314d19b1f164377a27ce10258c7d1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 15.796125110909543,\n",
       " 'counts': [128, 58, 28, 13],\n",
       " 'totals': [264, 254, 244, 235],\n",
       " 'precisions': [48.484848484848484,\n",
       "  22.834645669291337,\n",
       "  11.475409836065573,\n",
       "  5.531914893617022],\n",
       " 'bp': 0.9701515036966302,\n",
       " 'sys_len': 264,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "            num_beams=4,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1377e4967c4ca9a162b0d29b0acf05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "beam_search_hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(beam_search_hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 19.603385893047033,\n",
       " 'counts': [135, 64, 37, 20],\n",
       " 'totals': [267, 257, 247, 237],\n",
       " 'precisions': [50.561797752808985,\n",
       "  24.90272373540856,\n",
       "  14.979757085020243,\n",
       "  8.438818565400844],\n",
       " 'bp': 0.9814476614410015,\n",
       " 'sys_len': 267,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=beam_search_hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bad_words_ids\": [\n",
       "    [\n",
       "      62801\n",
       "    ]\n",
       "  ],\n",
       "  \"batch_size\": 2,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"decoder_start_token_id\": 62801,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 0,\n",
       "  \"max_length\": 512,\n",
       "  \"pad_token_id\": 62801,\n",
       "  \"use_cache\": false\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = {}\n",
    "\n",
    "\n",
    "def generate(inputs, decoder_input_ids):\n",
    "    global encoder_outputs\n",
    "    tokenizer_results = tokenizer(\n",
    "        inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    if not encoder_outputs.get(json.dumps(inputs)):\n",
    "        input_ids, attention_mask = (\n",
    "            tokenizer_results[\"input_ids\"],\n",
    "            tokenizer_results[\"attention_mask\"],\n",
    "        )\n",
    "        encoder_outputs[json.dumps(inputs)] = model.get_encoder()(\n",
    "            input_ids.repeat_interleave(\n",
    "                model.generation_config.num_beams,\n",
    "                dim=0,\n",
    "            ),\n",
    "            return_dict=True,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "    model_outputs = model(\n",
    "        **tokenizer_results,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        encoder_outputs=encoder_outputs[json.dumps(inputs)],\n",
    "    )\n",
    "    return model_outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_sequences_generator = GreedyGenerator(\n",
    "    use_tqdm=True,\n",
    "    device=model.device,\n",
    "    generate_fn=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0c46bdad7b48fd8e3fd67682325097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = greedy_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 15.796125110909543,\n",
       " 'counts': [128, 58, 28, 13],\n",
       " 'totals': [264, 254, 244, 235],\n",
       " 'precisions': [48.484848484848484,\n",
       "  22.834645669291337,\n",
       "  11.475409836065573,\n",
       "  5.531914893617022],\n",
       " 'bp': 0.9701515036966302,\n",
       " 'sys_len': 264,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_sequences_generator = BeamSearchGenerator(\n",
    "    use_tqdm=True,\n",
    "    beam_width=4,\n",
    "    device=model.device,\n",
    "    generate_fn=generate,\n",
    "    length_penalty_alpha=0.6,\n",
    "    minimum_penalty_tokens_length=5,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a03b71c6866448594206f9097442b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = beam_search_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 20.084845774979332,\n",
       " 'counts': [134, 66, 38, 21],\n",
       " 'totals': [262, 252, 242, 233],\n",
       " 'precisions': [51.14503816793893,\n",
       "  26.19047619047619,\n",
       "  15.702479338842975,\n",
       "  9.012875536480687],\n",
       " 'bp': 0.9625512774839297,\n",
       " 'sys_len': 262,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate-sequences",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
