{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "import datasets\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "from generate_sequences import GreedyGenerator, BeamSearchGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the translation model from transformers\n",
    "# model_name = \"Helsinki-NLP/opus-mt-ar-en\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "model_name = \"marefa-nlp/marefa-mt-en-ar\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "bleu_scorer = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "test_dataset = datasets.load_dataset('iwslt2017','iwslt2017-ar-en', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = 'en'\n",
    "target_language = 'ar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing on 10 samples only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " 10,\n",
       " [\"One major consequence of this work  is that maybe all of these decades,  we've had the whole concept of cybernetic revolt  in reverse.\",\n",
       "  \"It's not that machines first become intelligent  and then megalomaniacal  and try to take over the world.\",\n",
       "  \"It's quite the opposite,  that the urge to take control  of all possible futures  is a more fundamental principle  than that of intelligence,  that general intelligence may in fact emerge  directly from this sort of control-grabbing,  rather than vice versa.\",\n",
       "  'Another important consequence is goal seeking.',\n",
       "  \"I'm often asked, how does the ability to seek goals  follow from this sort of framework?\"],\n",
       " ['أحد العواقب الكبرى لهذا العمل هو أنه لربما طوال كل هذه العقود، كان لدينا المفهوم العكسي للثورة الآلية.',\n",
       "  'الأمر ليس في أن الآلات تصبح ذكية في البداية ثم ينتابها جنون العظمة و تحاول السيطرة على العالم.',\n",
       "  'إنه تماماً العكس، أن النزعة للسيطرة على كل الأزمنة المستقبلية الواردة هي مبدأ أساسي أكثر من مبدأ الذكاء، أن نواحي الذكاء العامة يمكن في الحقيقة أن تنبعث مباشرة من السيطرة، بدلاً من أن يكون الأمر بالعكس.',\n",
       "  'عاقبة أخرى مهمة هي البحث عن الهدف.',\n",
       "  'إنني أُسأل غالباً، كيف يمكن تفسير قدرة البحث عن الأهداف في هذا الإطار؟'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts = [example[source_language] for example in test_dataset['translation']][-10:]\n",
    "targets = [example[target_language] for example in test_dataset['translation']][-10:]\n",
    "len(input_texts), len(targets), input_texts[:5], targets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting `use_cache=False` as this disables optimizations being applied to transformers architecture [https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig.use_cache]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.num_beams=1\n",
    "model.generation_config.use_cache = False\n",
    "model.generation_config.batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(inputs,batch_size):\n",
    "    for i in tqdm(\n",
    "            range(0, len(inputs), batch_size),\n",
    "            desc=\"Generating Sequences\",\n",
    "            total=len(inputs) // batch_size,\n",
    "        ):\n",
    "        yield inputs[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate with Huggingface `generate` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Greedy method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting `do_sample=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            do_sample=False,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e756bab3b447b1971be30830f1db04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 15.796125110909543,\n",
       " 'counts': [128, 58, 28, 13],\n",
       " 'totals': [264, 254, 244, 235],\n",
       " 'precisions': [48.484848484848484,\n",
       "  22.834645669291337,\n",
       "  11.475409836065573,\n",
       "  5.531914893617022],\n",
       " 'bp': 0.9701515036966302,\n",
       " 'sys_len': 264,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With sampling and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ae01c324b24f1cb36722e660d4a45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 11.56429699624176,\n",
       " 'counts': [114, 43, 20, 9],\n",
       " 'totals': [280, 270, 260, 251],\n",
       " 'precisions': [40.714285714285715,\n",
       "  15.925925925925926,\n",
       "  7.6923076923076925,\n",
       "  3.585657370517928],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 280,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using beam search of width 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set explicitly `do_sample=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            num_beams=4,\n",
    "            do_sample=False,\n",
    "            length_penalty=0.6,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b5147db8804f809b1692a1d3aa94f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "beam_search_hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(beam_search_hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 20.084845774979332,\n",
       " 'counts': [134, 66, 38, 21],\n",
       " 'totals': [262, 252, 242, 233],\n",
       " 'precisions': [51.14503816793893,\n",
       "  26.19047619047619,\n",
       "  15.702479338842975,\n",
       "  9.012875536480687],\n",
       " 'bp': 0.9625512774839297,\n",
       " 'sys_len': 262,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=beam_search_hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial sampling and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            num_beams=4,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            length_penalty=0.6,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcd45c7df384321a91cae8ef22cbc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "beam_search_hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(beam_search_hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 20.84703501669694,\n",
       " 'counts': [133, 67, 40, 23],\n",
       " 'totals': [264, 254, 244, 235],\n",
       " 'precisions': [50.378787878787875,\n",
       "  26.37795275590551,\n",
       "  16.39344262295082,\n",
       "  9.787234042553191],\n",
       " 'bp': 0.9701515036966302,\n",
       " 'sys_len': 264,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=beam_search_hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate using generate-sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bad_words_ids\": [\n",
       "    [\n",
       "      62801\n",
       "    ]\n",
       "  ],\n",
       "  \"batch_size\": 2,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"decoder_start_token_id\": 62801,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 0,\n",
       "  \"max_length\": 512,\n",
       "  \"pad_token_id\": 62801,\n",
       "  \"use_cache\": false\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the generation function that is used for both, greedy and beam search generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = {}\n",
    "\n",
    "\n",
    "def generate(inputs, decoder_input_ids):\n",
    "    global encoder_outputs\n",
    "    tokenizer_results = tokenizer(\n",
    "        inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    if not encoder_outputs.get(json.dumps(inputs)):\n",
    "        input_ids, attention_mask = (\n",
    "            tokenizer_results[\"input_ids\"],\n",
    "            tokenizer_results[\"attention_mask\"],\n",
    "        )\n",
    "        encoder_outputs[json.dumps(inputs)] = model.get_encoder()(\n",
    "            input_ids.repeat_interleave(\n",
    "                model.generation_config.num_beams,\n",
    "                dim=0,\n",
    "            ),\n",
    "            return_dict=True,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "    model_outputs = model(\n",
    "        **tokenizer_results,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        encoder_outputs=encoder_outputs[json.dumps(inputs)],\n",
    "    )\n",
    "    return model_outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_sequences_generator = GreedyGenerator(\n",
    "    use_tqdm=True,\n",
    "    device=model.device,\n",
    "    generate_fn=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6033734ea5864ebfa0775704fbc3a77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = greedy_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 15.796125110909543,\n",
       " 'counts': [128, 58, 28, 13],\n",
       " 'totals': [264, 254, 244, 235],\n",
       " 'precisions': [48.484848484848484,\n",
       "  22.834645669291337,\n",
       "  11.475409836065573,\n",
       "  5.531914893617022],\n",
       " 'bp': 0.9701515036966302,\n",
       " 'sys_len': 264,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_sequences_generator = GreedyGenerator(\n",
    "    use_tqdm=True,\n",
    "    temperature=0.9,\n",
    "    device=model.device,\n",
    "    generate_fn=generate,\n",
    "    multinomial_sampling=True,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0524fe2f6a645f095a58ccf4c217c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = greedy_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 14.550224701069524,\n",
       " 'counts': [117, 50, 27, 14],\n",
       " 'totals': [280, 270, 260, 251],\n",
       " 'precisions': [41.785714285714285,\n",
       "  18.51851851851852,\n",
       "  10.384615384615385,\n",
       "  5.577689243027889],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 280,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_sequences_generator = BeamSearchGenerator(\n",
    "    beam_width=4,\n",
    "    use_tqdm=True,\n",
    "    length_penalty=0.6,\n",
    "    device=model.device,\n",
    "    generate_fn=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba26b0fc6184b3391a123f3227e2ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = beam_search_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 20.16216711910865,\n",
       " 'counts': [134, 67, 38, 21],\n",
       " 'totals': [261, 251, 241, 232],\n",
       " 'precisions': [51.34099616858238,\n",
       "  26.693227091633467,\n",
       "  15.767634854771785,\n",
       "  9.051724137931034],\n",
       " 'bp': 0.958730185172926,\n",
       " 'sys_len': 261,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial sampling and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_sequences_generator = BeamSearchGenerator(\n",
    "    beam_width=4,\n",
    "    use_tqdm=True,\n",
    "    temperature=0.9,\n",
    "    length_penalty=0.6,\n",
    "    device=model.device,\n",
    "    generate_fn=generate,\n",
    "    multinomial_sampling=True,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531562f99ce548b7ab1dc1db9f078a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = beam_search_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 17.835229766549798,\n",
       " 'counts': [130, 62, 32, 17],\n",
       " 'totals': [259, 249, 239, 230],\n",
       " 'precisions': [50.19305019305019,\n",
       "  24.899598393574298,\n",
       "  13.389121338912133,\n",
       "  7.391304347826087],\n",
       " 'bp': 0.951045807200927,\n",
       " 'sys_len': 259,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate-sequences",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
