{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "import datasets\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "from generate_sequences import GreedyGenerator, BeamSearchGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the translation model from transformers\n",
    "# model_name = \"Helsinki-NLP/opus-mt-ar-en\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "model_name = \"marefa-nlp/marefa-mt-en-ar\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "bleu_scorer = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "test_dataset = datasets.load_dataset('iwslt2017','iwslt2017-ar-en', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = 'en'\n",
    "target_language = 'ar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing on 10 samples only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " 10,\n",
       " [\"One major consequence of this work  is that maybe all of these decades,  we've had the whole concept of cybernetic revolt  in reverse.\",\n",
       "  \"It's not that machines first become intelligent  and then megalomaniacal  and try to take over the world.\",\n",
       "  \"It's quite the opposite,  that the urge to take control  of all possible futures  is a more fundamental principle  than that of intelligence,  that general intelligence may in fact emerge  directly from this sort of control-grabbing,  rather than vice versa.\",\n",
       "  'Another important consequence is goal seeking.',\n",
       "  \"I'm often asked, how does the ability to seek goals  follow from this sort of framework?\"],\n",
       " ['أحد العواقب الكبرى لهذا العمل هو أنه لربما طوال كل هذه العقود، كان لدينا المفهوم العكسي للثورة الآلية.',\n",
       "  'الأمر ليس في أن الآلات تصبح ذكية في البداية ثم ينتابها جنون العظمة و تحاول السيطرة على العالم.',\n",
       "  'إنه تماماً العكس، أن النزعة للسيطرة على كل الأزمنة المستقبلية الواردة هي مبدأ أساسي أكثر من مبدأ الذكاء، أن نواحي الذكاء العامة يمكن في الحقيقة أن تنبعث مباشرة من السيطرة، بدلاً من أن يكون الأمر بالعكس.',\n",
       "  'عاقبة أخرى مهمة هي البحث عن الهدف.',\n",
       "  'إنني أُسأل غالباً، كيف يمكن تفسير قدرة البحث عن الأهداف في هذا الإطار؟'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts = [example[source_language] for example in test_dataset['translation']][-10:]\n",
    "targets = [example[target_language] for example in test_dataset['translation']][-10:]\n",
    "len(input_texts), len(targets), input_texts[:5], targets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting `use_cache=False` as this disables optimizations being applied to transformers architecture [https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig.use_cache]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.num_beams=1\n",
    "model.generation_config.use_cache = False\n",
    "model.generation_config.batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(inputs,batch_size):\n",
    "    for i in tqdm(\n",
    "            range(0, len(inputs), batch_size),\n",
    "            desc=\"Generating Sequences\",\n",
    "            total=len(inputs) // batch_size,\n",
    "        ):\n",
    "        yield inputs[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate with Huggingface `generate` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Greedy method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting `do_sample=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            do_sample=False,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21c690b98854a098fc213c986cdae58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 15.796125110909543,\n",
       " 'counts': [128, 58, 28, 13],\n",
       " 'totals': [264, 254, 244, 235],\n",
       " 'precisions': [48.484848484848484,\n",
       "  22.834645669291337,\n",
       "  11.475409836065573,\n",
       "  5.531914893617022],\n",
       " 'bp': 0.9701515036966302,\n",
       " 'sys_len': 264,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial sampling, top_k, top_p, and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            top_k=100,\n",
    "            top_p=0.8,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27619cff1ed4b7bb054e45a9df98465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 16.432731874855403,\n",
       " 'counts': [132, 60, 31, 14],\n",
       " 'totals': [277, 267, 257, 248],\n",
       " 'precisions': [47.65342960288809,\n",
       "  22.471910112359552,\n",
       "  12.062256809338521,\n",
       "  5.645161290322581],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 277,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using beam search of width 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set explicitly `do_sample=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            num_beams=4,\n",
    "            do_sample=False,\n",
    "            length_penalty=0.6,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d09a5ef6f15448295ee3a91e744e854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "beam_search_hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(beam_search_hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 20.084845774979332,\n",
       " 'counts': [134, 66, 38, 21],\n",
       " 'totals': [262, 252, 242, 233],\n",
       " 'precisions': [51.14503816793893,\n",
       "  26.19047619047619,\n",
       "  15.702479338842975,\n",
       "  9.012875536480687],\n",
       " 'bp': 0.9625512774839297,\n",
       " 'sys_len': 262,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=beam_search_hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial, top-p, top-k sampling and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            top_k=100,\n",
    "            top_p=0.8,\n",
    "            num_beams=4,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            length_penalty=0.6,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed5415206ab4e7db92c1f78ac4662a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "beam_search_hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(beam_search_hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 21.792077490044267,\n",
       " 'counts': [135, 69, 42, 25],\n",
       " 'totals': [262, 252, 242, 233],\n",
       " 'precisions': [51.52671755725191,\n",
       "  27.38095238095238,\n",
       "  17.355371900826448,\n",
       "  10.729613733905579],\n",
       " 'bp': 0.9625512774839297,\n",
       " 'sys_len': 262,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=beam_search_hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate using generate-sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bad_words_ids\": [\n",
       "    [\n",
       "      62801\n",
       "    ]\n",
       "  ],\n",
       "  \"batch_size\": 2,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"decoder_start_token_id\": 62801,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 0,\n",
       "  \"max_length\": 512,\n",
       "  \"pad_token_id\": 62801,\n",
       "  \"use_cache\": false\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the generation function that is used for both, greedy and beam search generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = {}\n",
    "\n",
    "\n",
    "def generate(inputs, decoder_input_ids):\n",
    "    global encoder_outputs\n",
    "    tokenizer_results = tokenizer(\n",
    "        inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    if not encoder_outputs.get(json.dumps(inputs)):\n",
    "        input_ids, attention_mask = (\n",
    "            tokenizer_results[\"input_ids\"],\n",
    "            tokenizer_results[\"attention_mask\"],\n",
    "        )\n",
    "        encoder_outputs[json.dumps(inputs)] = model.get_encoder()(\n",
    "            input_ids.repeat_interleave(\n",
    "                model.generation_config.num_beams,\n",
    "                dim=0,\n",
    "            ),\n",
    "            return_dict=True,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "    model_outputs = model(\n",
    "        **tokenizer_results,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        encoder_outputs=encoder_outputs[json.dumps(inputs)],\n",
    "    )\n",
    "    return model_outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_sequences_generator = GreedyGenerator(\n",
    "    use_tqdm=True,\n",
    "    sort_samples=True,\n",
    "    device=model.device,\n",
    "    generation_forward=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0101acb14924ebc9b74f899d1a28095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = greedy_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 15.796125110909543,\n",
       " 'counts': [128, 58, 28, 13],\n",
       " 'totals': [264, 254, 244, 235],\n",
       " 'precisions': [48.484848484848484,\n",
       "  22.834645669291337,\n",
       "  11.475409836065573,\n",
       "  5.531914893617022],\n",
       " 'bp': 0.9701515036966302,\n",
       " 'sys_len': 264,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial and top-k, top-p sampling, and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_sequences_generator = GreedyGenerator(\n",
    "    use_tqdm=True,\n",
    "    temperature=0.9,\n",
    "    sort_samples=True,\n",
    "    top_k_sampling=100,\n",
    "    top_p_sampling=0.8,\n",
    "    device=model.device,\n",
    "    multinomial_sampling=True,\n",
    "    generation_forward=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c36cc8e29a4cd6bca9f30164f0a071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = greedy_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 16.963549254009152,\n",
       " 'counts': [133, 61, 31, 16],\n",
       " 'totals': [279, 269, 259, 250],\n",
       " 'precisions': [47.67025089605735, 22.676579925650557, 11.96911196911197, 6.4],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 279,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_sequences_generator = BeamSearchGenerator(\n",
    "    beam_width=4,\n",
    "    use_tqdm=True,\n",
    "    sort_samples=True,\n",
    "    length_penalty=0.6,\n",
    "    device=model.device,\n",
    "    generation_forward=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3f41dcd2344d669478305a523a112d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = beam_search_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 20.16216711910865,\n",
       " 'counts': [134, 67, 38, 21],\n",
       " 'totals': [261, 251, 241, 232],\n",
       " 'precisions': [51.34099616858238,\n",
       "  26.693227091633467,\n",
       "  15.767634854771785,\n",
       "  9.051724137931034],\n",
       " 'bp': 0.958730185172926,\n",
       " 'sys_len': 261,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial, top-p,top-k sampling, and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_sequences_generator = BeamSearchGenerator(\n",
    "    beam_width=4,\n",
    "    use_tqdm=True,\n",
    "    temperature=0.9,\n",
    "    sort_samples=True,\n",
    "    top_k_sampling=100,\n",
    "    length_penalty=0.6,\n",
    "    top_p_sampling=0.8,\n",
    "    device=model.device,\n",
    "    multinomial_sampling=True,\n",
    "    generation_forward=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb933bc8db5f4efc9491dfecb620c4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = beam_search_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 20.760619867696708,\n",
       " 'counts': [136, 69, 39, 22],\n",
       " 'totals': [260, 250, 240, 231],\n",
       " 'precisions': [52.30769230769231, 27.6, 16.25, 9.523809523809524],\n",
       " 'bp': 0.954895043959762,\n",
       " 'sys_len': 260,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate-sequences",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
