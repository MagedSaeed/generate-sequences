{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "import datasets\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "from generate_sequences.generate import GreedyGenerator, BeamSearchGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the translation model from transformers\n",
    "# model_name = \"Helsinki-NLP/opus-mt-ar-en\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "model_name = \"marefa-nlp/marefa-mt-en-ar\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "bleu_scorer = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "test_dataset = datasets.load_dataset('iwslt2017','iwslt2017-ar-en', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = 'en'\n",
    "target_language = 'ar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing on 10 samples only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " 10,\n",
       " [\"One major consequence of this work  is that maybe all of these decades,  we've had the whole concept of cybernetic revolt  in reverse.\",\n",
       "  \"It's not that machines first become intelligent  and then megalomaniacal  and try to take over the world.\",\n",
       "  \"It's quite the opposite,  that the urge to take control  of all possible futures  is a more fundamental principle  than that of intelligence,  that general intelligence may in fact emerge  directly from this sort of control-grabbing,  rather than vice versa.\",\n",
       "  'Another important consequence is goal seeking.',\n",
       "  \"I'm often asked, how does the ability to seek goals  follow from this sort of framework?\"],\n",
       " ['أحد العواقب الكبرى لهذا العمل هو أنه لربما طوال كل هذه العقود، كان لدينا المفهوم العكسي للثورة الآلية.',\n",
       "  'الأمر ليس في أن الآلات تصبح ذكية في البداية ثم ينتابها جنون العظمة و تحاول السيطرة على العالم.',\n",
       "  'إنه تماماً العكس، أن النزعة للسيطرة على كل الأزمنة المستقبلية الواردة هي مبدأ أساسي أكثر من مبدأ الذكاء، أن نواحي الذكاء العامة يمكن في الحقيقة أن تنبعث مباشرة من السيطرة، بدلاً من أن يكون الأمر بالعكس.',\n",
       "  'عاقبة أخرى مهمة هي البحث عن الهدف.',\n",
       "  'إنني أُسأل غالباً، كيف يمكن تفسير قدرة البحث عن الأهداف في هذا الإطار؟'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts = [example[source_language] for example in test_dataset['translation']][-10:]\n",
    "targets = [example[target_language] for example in test_dataset['translation']][-10:]\n",
    "len(input_texts), len(targets), input_texts[:5], targets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting `use_cache=False` as this disables optimizations being applied to transformers architecture [https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig.use_cache]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.num_beams=1\n",
    "model.generation_config.use_cache = False\n",
    "model.generation_config.batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(inputs,batch_size):\n",
    "    for i in tqdm(\n",
    "            range(0, len(inputs), batch_size),\n",
    "            desc=\"Generating Sequences\",\n",
    "            total=len(inputs) // batch_size,\n",
    "        ):\n",
    "        yield inputs[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate with Huggingface `generate` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Greedy method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting `do_sample=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            do_sample=False,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af9bf7dc28d423d928a6a078990ebfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 15.796125110909543,\n",
       " 'counts': [128, 58, 28, 13],\n",
       " 'totals': [264, 254, 244, 235],\n",
       " 'precisions': [48.484848484848484,\n",
       "  22.834645669291337,\n",
       "  11.475409836065573,\n",
       "  5.531914893617022],\n",
       " 'bp': 0.9701515036966302,\n",
       " 'sys_len': 264,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial sampling, top_k, and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            top_k=10,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bae91be237f468a990422d53e7d2db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 11.817608104701906,\n",
       " 'counts': [114, 41, 19, 11],\n",
       " 'totals': [281, 271, 261, 252],\n",
       " 'precisions': [40.569395017793596,\n",
       "  15.129151291512915,\n",
       "  7.2796934865900385,\n",
       "  4.365079365079365],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 281,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using beam search of width 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set explicitly `do_sample=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            num_beams=4,\n",
    "            do_sample=False,\n",
    "            length_penalty=0.6,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893e8388b5164202a682dc714390db89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "beam_search_hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(beam_search_hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 20.084845774979332,\n",
       " 'counts': [134, 66, 38, 21],\n",
       " 'totals': [262, 252, 242, 233],\n",
       " 'precisions': [51.14503816793893,\n",
       "  26.19047619047619,\n",
       "  15.702479338842975,\n",
       "  9.012875536480687],\n",
       " 'bp': 0.9625512774839297,\n",
       " 'sys_len': 262,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=beam_search_hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial sampling and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            top_k=10,\n",
    "            num_beams=4,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            length_penalty=0.6,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d3330948b648af9ba174a9b7ebe315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "beam_search_hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(beam_search_hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 18.750232296935486,\n",
       " 'counts': [133, 64, 35, 18],\n",
       " 'totals': [263, 253, 243, 234],\n",
       " 'precisions': [50.57034220532319,\n",
       "  25.296442687747035,\n",
       "  14.40329218106996,\n",
       "  7.6923076923076925],\n",
       " 'bp': 0.9663583678843662,\n",
       " 'sys_len': 263,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=beam_search_hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate using generate-sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bad_words_ids\": [\n",
       "    [\n",
       "      62801\n",
       "    ]\n",
       "  ],\n",
       "  \"batch_size\": 2,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"decoder_start_token_id\": 62801,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 0,\n",
       "  \"max_length\": 512,\n",
       "  \"pad_token_id\": 62801,\n",
       "  \"use_cache\": false\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the generation function that is used for both, greedy and beam search generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = {}\n",
    "\n",
    "\n",
    "def generate(inputs, decoder_input_ids):\n",
    "    global encoder_outputs\n",
    "    tokenizer_results = tokenizer(\n",
    "        inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    if not encoder_outputs.get(json.dumps(inputs)):\n",
    "        input_ids, attention_mask = (\n",
    "            tokenizer_results[\"input_ids\"],\n",
    "            tokenizer_results[\"attention_mask\"],\n",
    "        )\n",
    "        encoder_outputs[json.dumps(inputs)] = model.get_encoder()(\n",
    "            input_ids.repeat_interleave(\n",
    "                model.generation_config.num_beams,\n",
    "                dim=0,\n",
    "            ),\n",
    "            return_dict=True,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "    model_outputs = model(\n",
    "        **tokenizer_results,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        encoder_outputs=encoder_outputs[json.dumps(inputs)],\n",
    "    )\n",
    "    return model_outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_sequences_generator = GreedyGenerator(\n",
    "    use_tqdm=True,\n",
    "    device=model.device,\n",
    "    generation_forward=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31fe3cac4b5a4fac9ff579c140b27e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = greedy_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 15.796125110909543,\n",
       " 'counts': [128, 58, 28, 13],\n",
       " 'totals': [264, 254, 244, 235],\n",
       " 'precisions': [48.484848484848484,\n",
       "  22.834645669291337,\n",
       "  11.475409836065573,\n",
       "  5.531914893617022],\n",
       " 'bp': 0.9701515036966302,\n",
       " 'sys_len': 264,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial and top-k sampling, and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_sequences_generator = GreedyGenerator(\n",
    "    use_tqdm=True,\n",
    "    temperature=0.9,\n",
    "    top_k_sampling=10,\n",
    "    device=model.device,\n",
    "    generation_forward=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664e29cbd91047df8bfd206c46f805bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = greedy_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 15.290202908392255,\n",
       " 'counts': [128, 55, 26, 13],\n",
       " 'totals': [269, 259, 249, 240],\n",
       " 'precisions': [47.58364312267658,\n",
       "  21.235521235521237,\n",
       "  10.441767068273093,\n",
       "  5.416666666666667],\n",
       " 'bp': 0.9889095412986662,\n",
       " 'sys_len': 269,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_sequences_generator = GreedyGenerator(\n",
    "    use_tqdm=True,\n",
    "    temperature=0.9,\n",
    "    device=model.device,\n",
    "    multinomial_sampling=True,\n",
    "    generation_forward=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e080700a6c43219459c24adc812b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = greedy_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 11.027767060591364,\n",
       " 'counts': [113, 45, 18, 7],\n",
       " 'totals': [268, 258, 248, 238],\n",
       " 'precisions': [42.16417910447761,\n",
       "  17.441860465116278,\n",
       "  7.258064516129032,\n",
       "  2.9411764705882355],\n",
       " 'bp': 0.9851854581626466,\n",
       " 'sys_len': 268,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial and top-k sampling and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_sequences_generator = BeamSearchGenerator(\n",
    "    beam_width=4,\n",
    "    use_tqdm=True,\n",
    "    temperature=0.9,\n",
    "    top_k_sampling=10,\n",
    "    length_penalty=0.6,\n",
    "    device=model.device,\n",
    "    generation_forward=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbfba1a688d49d7a110a4e5671bf93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = beam_search_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 18.570334401568594,\n",
       " 'counts': [130, 63, 35, 18],\n",
       " 'totals': [263, 253, 243, 234],\n",
       " 'precisions': [49.42965779467681,\n",
       "  24.90118577075099,\n",
       "  14.40329218106996,\n",
       "  7.6923076923076925],\n",
       " 'bp': 0.9663583678843662,\n",
       " 'sys_len': 263,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with sorting samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_sequences_generator = BeamSearchGenerator(\n",
    "    beam_width=4,\n",
    "    use_tqdm=True,\n",
    "    sort_samples=True,\n",
    "    length_penalty=0.6,\n",
    "    device=model.device,\n",
    "    generation_forward=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8b25b9a3804119957544271b134e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = beam_search_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 20.16216711910865,\n",
       " 'counts': [134, 67, 38, 21],\n",
       " 'totals': [261, 251, 241, 232],\n",
       " 'precisions': [51.34099616858238,\n",
       "  26.693227091633467,\n",
       "  15.767634854771785,\n",
       "  9.051724137931034],\n",
       " 'bp': 0.958730185172926,\n",
       " 'sys_len': 261,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial sampling and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_sequences_generator = BeamSearchGenerator(\n",
    "    beam_width=4,\n",
    "    use_tqdm=True,\n",
    "    temperature=0.9,\n",
    "    length_penalty=0.6,\n",
    "    device=model.device,\n",
    "    multinomial_sampling=True,\n",
    "    generation_forward=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b35e29527745659fc0f242a8f958f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = beam_search_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 19.281261751916453,\n",
       " 'counts': [130, 64, 36, 20],\n",
       " 'totals': [260, 250, 240, 231],\n",
       " 'precisions': [50.0, 25.6, 15.0, 8.658008658008658],\n",
       " 'bp': 0.954895043959762,\n",
       " 'sys_len': 260,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate-sequences",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
