{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohVPGfg9lWAT"
      },
      "source": [
        "# Generate-Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EARqxR3ElciH"
      },
      "source": [
        "`generate-sequences` is a package created to generate text from auto-regressive pytorch-based models without tears. You can think of it as [huggingface generation mixin](https://huggingface.co/docs/transformers/en/main_classes/text_generation) but for a pytorch model you built from scratch. No need to include it to huggingface echosystem to generate from your model. The package features greedy generation as well as beam search generation. Many sampling techniques are also supported."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEEOztwWmW5C"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLAjrSC3meaT"
      },
      "source": [
        "`generate-sequences` can be installed with pip as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JGIejswlUFS"
      },
      "outputs": [],
      "source": [
        "pip install -U generate-sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-FP5zYKm3ja"
      },
      "source": [
        "## encoder-decoder architecutres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR8thFTAtGj4"
      },
      "source": [
        "In encoder-decoder architecture, the typical use-case is that the model will receive encoder inputs first. These inputs will be passed as batch to the encoder in order to generate tokens from the decoder. The decoder will get the first token as the `decoder_start_token_id` then start generating untill generating the `eos_token_id` where this indicates the model is done generating with this sequence. However, it will continue generating for other sequences in the batch untill all reached the `eos_token_id` in which the generation stops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5vgtdZmmwov"
      },
      "source": [
        "You can generate from an encoder-decoder architecture using greedy approach as follows. This also applies for beam search generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MlJa03psEgQ"
      },
      "source": [
        "First, Prepare your encoder inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_c1V-k0sFe0"
      },
      "outputs": [],
      "source": [
        "# usually the sentences are enclosed with bos and eos sentences.\n",
        "encoder_sentences = [\n",
        "    '<bos> sentence 1 <eos>',\n",
        "    '<bos> sentence 2 <eos>',\n",
        "    '<bos> sentence 3 <eos>',\n",
        "    ...\n",
        "]\n",
        "# You can also handle the <bos> and <eos> in the tokenizer if you tokenizer supports that\n",
        "encoder_inputs = tokenizer.tokenize(encoder_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8RXIlTCvCoB"
      },
      "source": [
        "Then, you need to tell the package how to get the logits from your model at each time step while generating. That is, you many define a method that takes the encoder and decoder inputs, and your model will generate the logits and return them. Usually, you will use the forward method of your model to get the logits, so, the recommended name of this method is `generation_forward` but you can name it literaly anything. This method can be as simple as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qne8UzPBmu_P"
      },
      "outputs": [],
      "source": [
        "model = MyModel(...)\n",
        "\n",
        "def generation_forward(encoder_inputs, decoder_inputs):\n",
        "  # do something when receiving the decoder inputs at each time step\n",
        "  logits = model(encoder_inputs, decoder_inputs)\n",
        "  return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N9yerqCo4iy"
      },
      "source": [
        "then, define the generator as follows, whether being beam search or greedy generation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8lB0ZkXo66Q"
      },
      "outputs": [],
      "source": [
        "from generate_sequences import GreedyGenerator, BeamSearchGenerator\n",
        "\n",
        "generator = GreedyGenerator(\n",
        "    device=model.device, # make sure to have the same device as your model\n",
        "    batch_size=32, # number of samples to process at each time step\n",
        "    max_length=512, # output max length\n",
        "    generation_forward=generation_forward,\n",
        "    eos_token_id = 1, # replace this with your own\n",
        "    decoder_start_token_id=0, # replace this with your own\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5xU5K7Gss2F"
      },
      "source": [
        "Then generate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoMcm3rlstmZ"
      },
      "outputs": [],
      "source": [
        "generator.generate(encoder_inputs=encoder_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ozPb6UZwL6m"
      },
      "source": [
        "here is the full code in one chunk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4DNoIuTwNjC"
      },
      "outputs": [],
      "source": [
        "from generate_sequences import GreedyGenerator, BeamSearchGenerator\n",
        "\n",
        "# usually the sentences are enclosed with bos and eos sentences.\n",
        "encoder_sentences = [\n",
        "    '<bos> sentence 1 <eos>',\n",
        "    '<bos> sentence 2 <eos>',\n",
        "    '<bos> sentence 3 <eos>',\n",
        "    ...\n",
        "]\n",
        "# You can also handle the <bos> and <eos> in the tokenizer if you tokenizer supports that\n",
        "encoder_inputs = tokenizer.tokenize(encoder_sentences)\n",
        "\n",
        "model = MyModel(...)\n",
        "\n",
        "def generation_forward(encoder_inputs, decoder_inputs):\n",
        "  # do something when receiving the decoder inputs at each time step\n",
        "  logits = model(encoder_inputs, decoder_inputs)\n",
        "  return logits\n",
        "\n",
        "generator = GreedyGenerator(\n",
        "    device=model.device, # make sure to have the same device as your model\n",
        "    batch_size=32,\n",
        "    max_length=512,\n",
        "    generation_forward=generation_forward,\n",
        "    eos_token_id = 1, # replace this with your own\n",
        "    decoder_start_token_id=0, # replace this with your own\n",
        ")\n",
        "\n",
        "# generate\n",
        "generator.generate(encoder_inputs=encoder_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUpeZNGYxgQC"
      },
      "source": [
        "## decoder-only architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKkIxR0Cxq2O"
      },
      "source": [
        "In decoder-only architecture, the typical use-case is that the model will receive the decoder inputs at each time-step in order to generate the next tokens. If you want to generate sentences from scractch, you can prompt the decoder with the decoder_start_token_id then the package will continue generating untill reaching the eos_token_id. Here is a sample example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7saG7n-wxmPM"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    '<bos> sentence 1', # it is not expected the <eos> to be passed!\n",
        "    \"<bos>\" # you can also pass the bos only.\n",
        "]\n",
        "# You can also handle the <bos>  in the tokenizer if you tokenizer supports that\n",
        "decoder_inputs = tokenizer.tokenize(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50UO6Ig_zHke"
      },
      "source": [
        "as in the encoder-decoder architecutre, write your generation method as follows. Note that encoder_inputs are still passed but you really do not need to to anything with them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HPQoYotzNbs"
      },
      "outputs": [],
      "source": [
        "model = MyModel(...)\n",
        "\n",
        "def generation_forward(encoder_inputs, decoder_inputs):\n",
        "  # do something when receiving the decoder inputs at each time step\n",
        "  logits = model(decoder_inputs)\n",
        "  return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsGa7sR4zlaE"
      },
      "source": [
        "define your generator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgQuWDfRzmeS"
      },
      "outputs": [],
      "source": [
        "from generate_sequences import GreedyGenerator, BeamSearchGenerator\n",
        "\n",
        "generator = GreedyGenerator(\n",
        "    device=model.device, # make sure to have the same device as your model\n",
        "    batch_size=32, # number of samples to process at each time step\n",
        "    max_length=512, # output max length\n",
        "    generation_forward=generation_forward,\n",
        "    eos_token_id = 0, # replace this with your own\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmGOVrDHzrin"
      },
      "source": [
        "then generate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89VJ9VnHzsKA"
      },
      "outputs": [],
      "source": [
        "generator.generate(decoder_inputs=decoder_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZhHEGoLqN9V"
      },
      "source": [
        "If the inputs are a set of sentences that are not `<bos>` and `batch_size` is greater than 1, it is required for the inputs to be of the same shape. Pass your padding token to `pad_decoder_inputs` in the `generate` method. You can also set the padding side to `right` but THIS IS NOT a standard practice in such situation unless you know what you are doing. The typical padding side is `left` which is the default value for `decoder_inputs_padding_size` parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCMap69Wz-RU"
      },
      "source": [
        "here is the full code in one chunk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO3My54Kz_gu"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    '<bos> sentence 1', # it is not expected the <eos> to be passed!\n",
        "    \"<bos>\" # you can also pass the bos only.\n",
        "]\n",
        "# You can also handle the <bos>  in the tokenizer if you tokenizer supports that\n",
        "decoder_inputs = tokenizer.tokenize(sentences)\n",
        "\n",
        "model = MyModel(...)\n",
        "\n",
        "def generation_forward(encoder_inputs, decoder_inputs):\n",
        "  # do something when receiving the decoder inputs at each time step\n",
        "  logits = model(decoder_inputs)\n",
        "  return logits\n",
        "\n",
        "from generate_sequences import GreedyGenerator, BeamSearchGenerator\n",
        "\n",
        "generator = GreedyGenerator(\n",
        "    device=model.device, # make sure to have the same device as your model\n",
        "    batch_size=32, # number of samples to process at each time step\n",
        "    max_length=512, # output max length\n",
        "    generation_forward=generation_forward,\n",
        "    eos_token_id = 1, # replace this with your own\n",
        "    decoder_start_token_id=0, # replace this with your own\n",
        ")\n",
        "\n",
        "# generate\n",
        "generator.generate(decoder_inputs=decoder_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1C7bflioq79"
      },
      "source": [
        "## Additional parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9Zks13C0OZr"
      },
      "source": [
        "Below are some useful parameters to be passed to the generator. These parameters can be used regardless of the generation method used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdP7czLo0b2f"
      },
      "source": [
        "### Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrBSQZ_x0eKF"
      },
      "source": [
        "`generate-sequences` support various sampling methods. To get an idea on sampling, I strongly advice to read this great article (https://huggingface.co/blog/how-to-generate) for more details on generation methods. Consider going over the following points for an overview:\n",
        "\n",
        "- Setting `multinomial_sampling=True` will generate tokens based on multinomial distribution instead of the default greedy approach.\n",
        "- You can play with the `temperature` by passing a value between 0,1.\n",
        "- `top_k_sampling` and `top_p_sampling` are also supported."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjLv-8IAznP_"
      },
      "source": [
        "### `sort_inputs_by_size`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrMudukbzqKW"
      },
      "source": [
        "Usually, inputs comes with various lengths. However, this is ineffecient as the padding will always consider the largest sample in the batch. If samples are ordered, then largest samples will be in teh begining, taking more time at the beging and utilizing the padding effectively. As the generation paces over batches, it moves faster. This parameter is `True` by default. Usually, you do not want to set it to `False` unless you know what you are doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `return_logits`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By setting this parameter to True in the generator, two lists will be returned. The first list is the output ids. The second list is a list of tuples where each tuple is the output token id along with its logit value. Logits are useful for many usecases like calculating perplexity, for instance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
