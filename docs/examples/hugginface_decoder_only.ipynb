{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder-Only Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from generate_sequences import GreedyGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majed_alshaibani/.virtualenvs/generate-sequences/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "model_name = \"gpt2\"  # You can choose other variants like 'gpt2-medium', 'gpt2-large', 'gpt2-xl'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token=tokenizer.decode(model.generation_config.bos_token_id)\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts to generate\n",
    "input_texts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The quick brown fox\",\n",
    "    \"Last night I dreamed\",\n",
    "    \"In the heart of the city\",\n",
    "    \"At the edge of the world\",\n",
    "]\n",
    "MAX_LENGTH = 50\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(texts, batch_size):\n",
    "    \"\"\"Yield successive n-sized batches from texts.\"\"\"\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        yield texts[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text using HuggingFace `generate` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ec22749060413bb6fc3bdc5412cbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Texts: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Once upon a time\n",
      "Generated: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n",
      "\n",
      "Input: The quick brown fox\n",
      "Generated: The quick brown foxes are a great way to get a little bit of a kick out of your dog.\n",
      "\n",
      "The quick brown foxes are a great way to get a little bit of a kick out of your dog. The quick brown fox\n",
      "\n",
      "Input: Last night I dreamed\n",
      "Generated: Last night I dreamed of a day when I could go to the beach and swim with my friends. I was so excited to be back in the ocean. I was so excited to be back in the ocean. I was so excited to be\n",
      "\n",
      "Input: In the heart of the city\n",
      "Generated: In the heart of the city, the city of San Francisco is a city of people. It's a place where people come together to celebrate, to celebrate, to celebrate. It's a place where people come together to celebrate, to celebrate, to\n",
      "\n",
      "Input: At the edge of the world\n",
      "Generated: At the edge of the world, the world is a place of great beauty. The world is a place of great fear. The world is a place of great fear. The world is a place of great fear. The world is a place of great\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_texts = []\n",
    "for batch in tqdm(get_batches(input_texts, BATCH_SIZE), desc=\"Generating Texts\"):\n",
    "    # Tokenize batch\n",
    "    encoded_input = tokenizer(\n",
    "        batch,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate text\n",
    "    output = model.generate(\n",
    "        input_ids=encoded_input[\"input_ids\"],\n",
    "        attention_mask=encoded_input[\"attention_mask\"],\n",
    "        max_length=MAX_LENGTH,  # Max length of the generated text\n",
    "    )\n",
    "\n",
    "    # Decode generated texts\n",
    "    batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n",
    "    generated_texts.extend(batch_generated_texts)\n",
    "\n",
    "# Print all collected results\n",
    "for input_text, generated_text in zip(input_texts, generated_texts):\n",
    "    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b69863ff0b4903867487e2a5674e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Texts: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Once upon a time\n",
      "Generated: Once upon a time, you might have heard about the \"Halloween Horror\" phenomenon. The Halloween Horror is a Halloween film that was made for a Halloween convention in New York City and screened at the 2015 Halloween Horror Film Festival. The film had a\n",
      "\n",
      "Input: The quick brown fox\n",
      "Generated: The quick brown fox is a bit more challenging as the fox can't even be seen unless you're looking closely. The fox also has a tendency to go straight at you, and it's more difficult to get your eyes on the fox if you're\n",
      "\n",
      "Input: Last night I dreamed\n",
      "Generated: Last night I dreamed about being the first person to actually see what it was like to be in a place like this. It was amazing. I was so honored to be able to be a part of it. I really feel like I'm\n",
      "\n",
      "Input: In the heart of the city\n",
      "Generated: In the heart of the city is the Church of the Holy Trinity. The Trinity is the living God and Father of all things, the Savior of the world, the Creator and Ruler of all things.\n",
      "\n",
      "In the Bible, God is the Father\n",
      "\n",
      "Input: At the edge of the world\n",
      "Generated: At the edge of the world, the men of my village would come to my tent to meet me at the door.\n",
      "\n",
      "I said nothing.\n",
      "\n",
      "The men of my village were the men of the city. I said nothing.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_texts = []\n",
    "for batch in tqdm(get_batches(input_texts, BATCH_SIZE), desc=\"Generating Texts\"):\n",
    "    # Tokenize batch\n",
    "    encoded_input = tokenizer(batch, return_tensors=\"pt\", padding=True).to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    output = model.generate(\n",
    "        input_ids=encoded_input[\"input_ids\"],\n",
    "        attention_mask=encoded_input[\"attention_mask\"],\n",
    "        max_length=MAX_LENGTH,  # Max length of the generated text\n",
    "        top_k=50,  # Limits the sampling pool to the top_k tokens\n",
    "        top_p=0.95,  # Nucleus sampling: sample only from top_p probability mass\n",
    "        temperature=0.7,  # Sampling temperature: lower value -> more conservative, higher value -> more random\n",
    "        do_sample=True  # Enable sampling\n",
    "    )\n",
    "    \n",
    "    # Decode generated texts\n",
    "    batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n",
    "    generated_texts.extend(batch_generated_texts)\n",
    "    \n",
    "# Print all collected results\n",
    "for input_text, generated_text in zip(input_texts, generated_texts):\n",
    "    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with generate-sequences, greedy generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_forward(encoder_inputs, decoder_inputs):\n",
    "    return model(input_ids=decoder_inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_greedy_generator = GreedyGenerator(\n",
    "    use_tqdm=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=MAX_LENGTH,\n",
    "    device=model.device,\n",
    "    generation_forward=generation_forward,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb6960db21247f4af16a230bf5f6ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8af70a56dcf4132a9b17954950aee95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3cc6bb3aeb4ae29eb9fa2b0a6c9738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Once upon a time\n",
      "Generated: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n",
      "\n",
      "Input: The quick brown fox\n",
      "Generated: The quick brown foxes are a great way to get a little bit of a kick out of your dog.\n",
      "\n",
      "The quick brown foxes are a great way to get a little bit of a kick out of your dog. The quick brown fox\n",
      "\n",
      "Input: Last night I dreamed\n",
      "Generated: Last night I dreamed of a day when I could go to the beach and swim with my friends. I was so excited to see the ocean, the waves, the waves. I was so excited to see the ocean, the waves, the\n",
      "\n",
      "Input: In the heart of the city\n",
      "Generated: In the heart of the city, the city of San Francisco is a city of people. It's a place where people come together to celebrate, to celebrate, to celebrate. It's a place where people come together to celebrate, to celebrate, to\n",
      "\n",
      "Input: At the edge of the world\n",
      "Generated: At the edge of the world, the world is a place of great beauty. The world is a place of great fear. The world is a place of great fear. The world is a place of great fear. The world is a place of great\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_texts = []\n",
    "for batch in get_batches(input_texts, BATCH_SIZE):\n",
    "    # Tokenize batch\n",
    "    encoded_input = tokenizer(batch, return_tensors=\"pt\", padding=True).to(device)\n",
    "    # Generate text\n",
    "    output = gpt2_greedy_generator.generate(\n",
    "        encoder_inputs=None,\n",
    "        decoder_inputs=encoded_input[\"input_ids\"],\n",
    "        pad_decoder_inputs=tokenizer.bos_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode generated texts\n",
    "    batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n",
    "    generated_texts.extend(batch_generated_texts)\n",
    "    \n",
    "# Print all collected results\n",
    "for input_text, generated_text in zip(input_texts, generated_texts):\n",
    "    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate with generate-sequences, greedy with sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_forward(encoder_inputs, decoder_inputs):\n",
    "    return model(input_ids=decoder_inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_greedy_generator = GreedyGenerator(\n",
    "    use_tqdm=True,\n",
    "    top_k_sampling=50,\n",
    "    top_p_sampling=0.95,\n",
    "    device=model.device,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_length=MAX_LENGTH,\n",
    "    multinomial_sampling=True,\n",
    "    generation_forward=generation_forward,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee83e629287d4035af43cee9d6cfc753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eea5da2a9ba499ea9936f973351e90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973bbc58ffd24191971956c9bc6a709c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Once upon a time\n",
      "Generated: Once upon a time when every thought and emotion of the human mind was to be consumed by the same thought and emotion, we are confronted with a false and utterly ungrateful reality. Our ignorance is the only thing that can bring about the correct mental\n",
      "\n",
      "Input: The quick brown fox\n",
      "Generated: The quick brown fox out of the corner of my eye and I realised I'd found my spot on this list for the best price. My sister and I, our only child, just had started school in the summer of 2015 so we'd not been\n",
      "\n",
      "Input: Last night I dreamed\n",
      "Generated: Last night I dreamed about how beautiful and beautiful this summer's beautiful people were.\n",
      "\n",
      "The day after the premiere of my new book 'The End of History', I was having dinner in the park at the time of the premiere at the\n",
      "\n",
      "Input: In the heart of the city\n",
      "Generated: In the heart of the city, two-thirds of Chicago's schools don't have a superintendent.\n",
      "\n",
      "While the city offers some flexibility in terms of whether or not district leaders can appoint schools superintendent, the mayor says he is not making public education\n",
      "\n",
      "Input: At the edge of the world\n",
      "Generated: At the edge of the world, he was the first person on Earth who took on more energy. His heart didn't want anything to do with it. His body was empty as his body had been built in the beginning. It's because he has\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_texts = []\n",
    "for batch in get_batches(input_texts, BATCH_SIZE):\n",
    "    # Tokenize batch\n",
    "    encoded_input = tokenizer(batch, return_tensors=\"pt\", padding=True).to(device)\n",
    "    # Generate text\n",
    "    output = gpt2_greedy_generator.generate(\n",
    "        encoder_inputs=None,\n",
    "        decoder_inputs=encoded_input[\"input_ids\"],\n",
    "        pad_decoder_inputs=tokenizer.bos_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode generated texts\n",
    "    batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n",
    "    generated_texts.extend(batch_generated_texts)\n",
    "    \n",
    "# Print all collected results\n",
    "for input_text, generated_text in zip(input_texts, generated_texts):\n",
    "    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate-sequences",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
