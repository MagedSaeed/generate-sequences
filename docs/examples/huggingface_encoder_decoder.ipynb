{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "import datasets\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "from generate_sequences import GreedyGenerator, BeamSearchGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magedsaeed/virtualenvs/generate-sequences/lib/python3.12/site-packages/datasets/load.py:1486: FutureWarning: The repository for iwslt2017 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/iwslt2017\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the translation model from transformers\n",
    "# model_name = \"Helsinki-NLP/opus-mt-ar-en\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "model_name = \"marefa-nlp/marefa-mt-en-ar\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "bleu_scorer = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "test_dataset = datasets.load_dataset('iwslt2017','iwslt2017-ar-en', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = 'en'\n",
    "target_language = 'ar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing on 10 samples only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " 10,\n",
       " [\"One major consequence of this work  is that maybe all of these decades,  we've had the whole concept of cybernetic revolt  in reverse.\",\n",
       "  \"It's not that machines first become intelligent  and then megalomaniacal  and try to take over the world.\",\n",
       "  \"It's quite the opposite,  that the urge to take control  of all possible futures  is a more fundamental principle  than that of intelligence,  that general intelligence may in fact emerge  directly from this sort of control-grabbing,  rather than vice versa.\",\n",
       "  'Another important consequence is goal seeking.',\n",
       "  \"I'm often asked, how does the ability to seek goals  follow from this sort of framework?\"],\n",
       " ['أحد العواقب الكبرى لهذا العمل هو أنه لربما طوال كل هذه العقود، كان لدينا المفهوم العكسي للثورة الآلية.',\n",
       "  'الأمر ليس في أن الآلات تصبح ذكية في البداية ثم ينتابها جنون العظمة و تحاول السيطرة على العالم.',\n",
       "  'إنه تماماً العكس، أن النزعة للسيطرة على كل الأزمنة المستقبلية الواردة هي مبدأ أساسي أكثر من مبدأ الذكاء، أن نواحي الذكاء العامة يمكن في الحقيقة أن تنبعث مباشرة من السيطرة، بدلاً من أن يكون الأمر بالعكس.',\n",
       "  'عاقبة أخرى مهمة هي البحث عن الهدف.',\n",
       "  'إنني أُسأل غالباً، كيف يمكن تفسير قدرة البحث عن الأهداف في هذا الإطار؟'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts = [example[source_language] for example in test_dataset['translation']][-10:]\n",
    "targets = [example[target_language] for example in test_dataset['translation']][-10:]\n",
    "len(input_texts), len(targets), input_texts[:5], targets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation and utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting `use_cache=False` as this disables optimizations being applied to transformers architecture [https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig.use_cache]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.num_beams=1\n",
    "model.generation_config.use_cache = False\n",
    "model.generation_config.batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(inputs,batch_size):\n",
    "    for i in tqdm(\n",
    "            range(0, len(inputs), batch_size),\n",
    "            desc=\"Generating Sequences\",\n",
    "            total=len(inputs) // batch_size,\n",
    "        ):\n",
    "        yield inputs[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate with Huggingface `generate` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Greedy method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting `do_sample=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            do_sample=False,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1dd9f1380414d22ba131f5d7adece28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 15.796125110909543,\n",
       " 'counts': [128, 58, 28, 13],\n",
       " 'totals': [264, 254, 244, 235],\n",
       " 'precisions': [48.484848484848484,\n",
       "  22.834645669291337,\n",
       "  11.475409836065573,\n",
       "  5.531914893617022],\n",
       " 'bp': 0.9701515036966302,\n",
       " 'sys_len': 264,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial sampling, top_k, top_p, and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            top_k=100,\n",
    "            top_p=0.8,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47707537057948a0b40694f49f055d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 15.895432255918095,\n",
       " 'counts': [124, 56, 31, 13],\n",
       " 'totals': [267, 257, 248, 239],\n",
       " 'precisions': [46.441947565543074,\n",
       "  21.78988326848249,\n",
       "  12.5,\n",
       "  5.439330543933054],\n",
       " 'bp': 0.9814476614410015,\n",
       " 'sys_len': 267,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using beam search of width 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set explicitly `do_sample=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            num_beams=4,\n",
    "            do_sample=False,\n",
    "            length_penalty=0.6,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75080c46ad84b8a865d1170a879a355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "beam_search_hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(beam_search_hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 20.084845774979332,\n",
       " 'counts': [134, 66, 38, 21],\n",
       " 'totals': [262, 252, 242, 233],\n",
       " 'precisions': [51.14503816793893,\n",
       "  26.19047619047619,\n",
       "  15.702479338842975,\n",
       "  9.012875536480687],\n",
       " 'bp': 0.9625512774839297,\n",
       " 'sys_len': 262,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=beam_search_hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial, top-p, top-k sampling and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts):\n",
    "    translated_texts = list()\n",
    "    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n",
    "        translated_tokens = model.generate(\n",
    "            top_k=100,\n",
    "            top_p=0.8,\n",
    "            num_beams=4,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            length_penalty=0.6,\n",
    "            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n",
    "        )\n",
    "        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193174850b4d4a6495ea080272bf52ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example batch of input sentences\n",
    "beam_search_hf_predictions = translate(input_texts)\n",
    "len(input_texts), len(beam_search_hf_predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 20.198004312748168,\n",
       " 'counts': [135, 67, 38, 21],\n",
       " 'totals': [262, 252, 242, 233],\n",
       " 'precisions': [51.52671755725191,\n",
       "  26.58730158730159,\n",
       "  15.702479338842975,\n",
       "  9.012875536480687],\n",
       " 'bp': 0.9625512774839297,\n",
       " 'sys_len': 262,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=beam_search_hf_predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate using generate-sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bad_words_ids\": [\n",
       "    [\n",
       "      62801\n",
       "    ]\n",
       "  ],\n",
       "  \"batch_size\": 2,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"decoder_start_token_id\": 62801,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 0,\n",
       "  \"max_length\": 512,\n",
       "  \"pad_token_id\": 62801,\n",
       "  \"use_cache\": false\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the generation function that is used for both, greedy and beam search generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = {}\n",
    "\n",
    "\n",
    "def generate(inputs, decoder_input_ids):\n",
    "    global encoder_outputs\n",
    "    tokenizer_results = tokenizer(\n",
    "        inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    if not encoder_outputs.get(json.dumps(inputs)):\n",
    "        input_ids, attention_mask = (\n",
    "            tokenizer_results[\"input_ids\"],\n",
    "            tokenizer_results[\"attention_mask\"],\n",
    "        )\n",
    "        encoder_outputs[json.dumps(inputs)] = model.get_encoder()(\n",
    "            input_ids.repeat_interleave(\n",
    "                model.generation_config.num_beams,\n",
    "                dim=0,\n",
    "            ),\n",
    "            return_dict=True,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "    model_outputs = model(\n",
    "        **tokenizer_results,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        encoder_outputs=encoder_outputs[json.dumps(inputs)],\n",
    "    )\n",
    "    return model_outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_sequences_generator = GreedyGenerator(\n",
    "    use_tqdm=True,\n",
    "    sort_encoder_inputs=True,\n",
    "    device=model.device,\n",
    "    generation_forward=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e825f3f457e1405eb396cf4b541316db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = greedy_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 15.796125110909543,\n",
       " 'counts': [128, 58, 28, 13],\n",
       " 'totals': [264, 254, 244, 235],\n",
       " 'precisions': [48.484848484848484,\n",
       "  22.834645669291337,\n",
       "  11.475409836065573,\n",
       "  5.531914893617022],\n",
       " 'bp': 0.9701515036966302,\n",
       " 'sys_len': 264,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial and top-k, top-p sampling, and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_sequences_generator = GreedyGenerator(\n",
    "    use_tqdm=True,\n",
    "    temperature=0.9,\n",
    "    top_k_sampling=100,\n",
    "    top_p_sampling=0.8,\n",
    "    device=model.device,\n",
    "    sort_encoder_inputs=True,\n",
    "    multinomial_sampling=True,\n",
    "    generation_forward=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bff8e14e0fd4e26aed26e5e4045ba61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = greedy_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 18.537916023808666,\n",
       " 'counts': [132, 62, 33, 19],\n",
       " 'totals': [266, 256, 246, 237],\n",
       " 'precisions': [49.62406015037594,\n",
       "  24.21875,\n",
       "  13.414634146341463,\n",
       "  8.016877637130802],\n",
       " 'bp': 0.9776961023999414,\n",
       " 'sys_len': 266,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_sequences_generator = BeamSearchGenerator(\n",
    "    beam_width=4,\n",
    "    use_tqdm=True,\n",
    "    length_penalty=0.6,\n",
    "    device=model.device,\n",
    "    sort_encoder_inputs=True,\n",
    "    generation_forward=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5df0c886512404096d48987e79b7e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = beam_search_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 20.16216711910865,\n",
       " 'counts': [134, 67, 38, 21],\n",
       " 'totals': [261, 251, 241, 232],\n",
       " 'precisions': [51.34099616858238,\n",
       "  26.693227091633467,\n",
       "  15.767634854771785,\n",
       "  9.051724137931034],\n",
       " 'bp': 0.958730185172926,\n",
       " 'sys_len': 261,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With multinomial, top-p,top-k sampling, and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_sequences_generator = BeamSearchGenerator(\n",
    "    beam_width=4,\n",
    "    use_tqdm=True,\n",
    "    temperature=0.9,\n",
    "    top_k_sampling=100,\n",
    "    length_penalty=0.6,\n",
    "    top_p_sampling=0.8,\n",
    "    device=model.device,\n",
    "    sort_encoder_inputs=True,\n",
    "    multinomial_sampling=True,\n",
    "    generation_forward=generate,\n",
    "    batch_size=model.generation_config.batch_size,\n",
    "    max_length=model.generation_config.max_length,\n",
    "    eos_token_id=model.generation_config.eos_token_id,\n",
    "    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c548092f30a444fead46872e9644a13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Sequences:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ids = beam_search_sequences_generator.generate(input_texts)\n",
    "predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n",
    "len(input_texts), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 21.700676010280976,\n",
       " 'counts': [134, 70, 41, 25],\n",
       " 'totals': [261, 251, 241, 232],\n",
       " 'precisions': [51.34099616858238,\n",
       "  27.888446215139442,\n",
       "  17.012448132780083,\n",
       "  10.775862068965518],\n",
       " 'bp': 0.958730185172926,\n",
       " 'sys_len': 261,\n",
       " 'ref_len': 272}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(predictions=predictions, references=targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate-sequences",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
