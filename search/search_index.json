{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udd22 Generate Sequences","text":"<p><code>generate-sequences</code> is a Python library for generating sequences from deep learning architectures with support for greedy search, beam search, and customizable configurations.</p> <p>This package generates sequences from architectures developed with PyTorch. It can <code>generate</code> in a greedy manner or using beam search. It can <code>generate</code> from both decoder-only and encoder-decoder architectures.</p> <ul> <li>Introduction and Getting Started: This README.md, also: https://magedsaeed.github.io/generate-sequences/Getting%20Started</li> <li>Examples: https://magedsaeed.github.io/generate-sequences/examples/huggingface_encoder_decoder</li> <li>ChangeLog: https://magedsaeed.github.io/generate-sequences/CHANGELOG</li> </ul>"},{"location":"#introduction","title":"\ud83d\ude80 Introduction","text":"<p>Welcome to Generate Sequences, a Python package designed for generating sequences using popular decoding strategies. Whether the system is a text generation, language modeling, machine translation, or any task requiring sequence decoding, this library provides a simple interface to implement and customize the sequence generation pipelines.</p>"},{"location":"#motivation","title":"\ud83c\udfaf Motivation","text":"<p>Hugging Face\u2019s <code>model.generate</code> method is a great tool for generating sequences from LLMs. However, in order to use this method, the developed model needs to be a Hugging Face model adhering to specific constraints depending on the model architecture. This package generalizes that approach by introducing a <code>generation_forward</code> method, where you specify how the model outputs sequences (this is the part that differs from model to model). Other parts are standardized across both beam search and greedy search. Additionally, this package supports popular generation methods such as sampling, temperature, top-p sampling, and top-k sampling.</p>"},{"location":"#features","title":"\u2728 Features","text":"<ul> <li>Greedy Search: Generate sequences by selecting the most probable token at each step.</li> <li>Beam Search: Explore multiple hypotheses to generate the best possible sequence.</li> <li>Custom Configurations: Fine-tune decoding parameters to suit your specific task.</li> <li>Lightweight &amp; Modular: Easy-to-read code designed with flexibility in mind.</li> </ul>"},{"location":"#directory-structure","title":"\ud83d\udcc2 Directory Structure","text":"<pre><code>generate-sequences/\n\u251c\u2500\u2500 docs/                     # Documentation &amp; Examples\n\u2502   \u251c\u2500\u2500 examples/\n\u2502   \u2502   \u251c\u2500\u2500 huggingface_decoder_only.ipynb\n\u2502   \u2502   \u251c\u2500\u2500 huggingface_encoder_decoder.ipynb\n\u2502   \u251c\u2500\u2500 CHANGELOG.md\n\u2502   \u251c\u2500\u2500 Getting Started.ipynb\n\u2502   \u251c\u2500\u2500 index.md\n\u2502\n\u251c\u2500\u2500 generate_sequences/        # Core Library\n\u2502   \u251c\u2500\u2500 generate/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 base.py\n\u2502   \u2502   \u251c\u2500\u2500 beam_search.py\n\u2502   \u2502   \u251c\u2500\u2500 greedy.py\n\u2502   \u251c\u2500\u2500 utils.py\n\u2502   \u251c\u2500\u2500 version.py\n\u2502   \u251c\u2500\u2500 py.typed\n\u2502\n\u251c\u2500\u2500 scripts/                   # Utility scripts\n\u2502   \u251c\u2500\u2500 prepare_changelog.py\n\u2502   \u251c\u2500\u2500 release_notes.py\n\u2502   \u251c\u2500\u2500 release.sh\n\u2502\n\u251c\u2500\u2500 tests/                     # Unit Tests\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 generate_test.py\n\u2502   \u251c\u2500\u2500 utils_test.py\n\u2502\n\u251c\u2500\u2500 .github/                   # GitHub Actions &amp; Workflows\n\u251c\u2500\u2500 LICENSE                    # Project License\n\u251c\u2500\u2500 pyproject.toml             # Project dependencies &amp; settings\n\u251c\u2500\u2500 README.md                  # Project documentation\n\u251c\u2500\u2500 RELEASE_PROCESS.md         # Release workflow guide\n\u2514\u2500\u2500 mkdocs.yml                 # Documentation configuration\n</code></pre>"},{"location":"#installation","title":"\ud83d\udee0\ufe0f Installation","text":"<p>The package is installable through pip:</p> <pre><code>pip install -U generate-sequences\n</code></pre> <p>You can also install the package from the latest commit on GitHub by cloning the repository and installing it manually:</p> <pre><code>git clone https://github.com/MagedSaeed/generate-sequences.git\ncd generate-sequences\npip install -e .\n</code></pre>"},{"location":"#usage","title":"\ud83d\udca1 Usage","text":"<p>You can always go to the getting-started page here: https://magedsaeed.github.io/generate-sequences/Getting%20Started/. However, this provides a TLDR overview of how the package works.</p> <p>First, import the generators:</p> <pre><code>from generate_sequences import GreedyGenerator, BeamSearchGenerator\n</code></pre> <p>Then, specify how the package should generate from your model given the encoder and decoder inputs and return the model logits. This method takes as an argument the encoder <code>inputs</code> (can be None for decoder-only architecture) and the decoder <code>decoder_input_ids</code> at a given time step.</p> <p>This method can be as simple as:</p> <pre><code>def generate(inputs, decoder_input_ids):\n    tokenizer_results = tokenizer(\n        inputs,\n        return_tensors=\"pt\",\n        padding=True,\n    )\n    model_outputs = model(**tokenizer_results)\n    return model_outputs.logits\n</code></pre> <p>Or more advanced with a caching mechanism in encoder-decoder architectures where the encoder outputs are cached:</p> <pre><code>encoder_outputs = {}\n\ndef generate(inputs, decoder_input_ids):\n    global encoder_outputs\n    tokenizer_results = tokenizer(\n        inputs,\n        return_tensors=\"pt\",\n        padding=True,\n    )\n    if not encoder_outputs.get(json.dumps(inputs)):\n        input_ids, attention_mask = (\n            tokenizer_results[\"input_ids\"],\n            tokenizer_results[\"attention_mask\"],\n        )\n        encoder_outputs[json.dumps(inputs)] = model.get_encoder()(\n            input_ids.repeat_interleave(\n                model.generation_config.num_beams,\n                dim=0,\n            ),\n            return_dict=True,\n            attention_mask=attention_mask,\n        )\n    model_outputs = model(\n        **tokenizer_results,\n        decoder_input_ids=decoder_input_ids,\n        encoder_outputs=encoder_outputs[json.dumps(inputs)],\n    )\n    return model_outputs.logits\n</code></pre> <p>Then, you can perform a greedy search or beam search as follows:</p> <ul> <li>Greedy Search</li> </ul> <pre><code>greedy_sequences_generator = GreedyGenerator(\n    use_tqdm=True,\n    sort_inputs_by_size=True,\n    device=model.device,\n    generation_forward=generate,\n    batch_size=model.generation_config.batch_size,\n    max_length=model.generation_config.max_length,\n    eos_token_id=model.generation_config.eos_token_id,\n    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n)\n\nprediction_ids = greedy_sequences_generator.generate(input_texts)\npredictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n</code></pre> <ul> <li>Beam Search</li> </ul> <pre><code>beam_search_sequences_generator = BeamSearchGenerator(\n    beam_width=4,\n    use_tqdm=True,\n    length_penalty=0.6,\n    device=model.device,\n    sort_inputs_by_size=True,\n    generation_forward=generate,\n    batch_size=model.generation_config.batch_size,\n    max_length=model.generation_config.max_length,\n    eos_token_id=model.generation_config.eos_token_id,\n    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n)\n\nprediction_ids = beam_search_sequences_generator.generate(input_texts)\npredictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\n</code></pre>"},{"location":"#running-tests","title":"\ud83e\uddea Running Tests","text":"<p>Ensure everything works as expected by running the unit tests:</p> <pre><code>pytest tests/\n</code></pre>"},{"location":"#documentation","title":"\ud83d\udcd6 Documentation","text":"<p>You can explore the full docs here: https://magedsaeed.github.io/generate-sequences.</p> <p>Docs were built using this great material-mkdocs template: https://squidfunk.github.io/mkdocs-material/.</p> <p>You can also navigate through the repository to:  - Getting Started: Check out <code>docs/Getting Started.ipynb</code> for a step-by-step guide. - Examples: Notebooks demonstrating integration with Hugging Face models are available in <code>docs/examples/</code></p>"},{"location":"#changelog","title":"\ud83d\udcdc ChangeLog","text":"<p>You can find a detailed list of changes and updates in the ChangeLog. This document keeps track of new features, bug fixes, and other improvements.</p>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! To contribute:</p> <ol> <li>Fork the repository.</li> <li>Create a new branch (<code>git checkout -b feature-name</code>).</li> <li>Commit your changes (<code>git commit -m \"Add feature\"</code>).</li> <li>Push to the branch (<code>git push origin feature-name</code>).</li> <li>Open a pull request.</li> </ol> <p>Please ensure your code follows the existing style and includes appropriate tests.</p>"},{"location":"#license","title":"\ud83d\udee1\ufe0f License","text":"<p>This project is licensed under the Apache License. See the LICENSE file for details.</p>"},{"location":"#acknowledgments","title":"\ud83c\udf1f Acknowledgments","text":"<p>Special thanks to the open-source community for inspiring this project \ud83d\ude4c. I want to give the credit here to Allen AI Python Package Template (https://github.com/allenai/python-package-template), which provides excellent functionality including GitHub Actions with automated testing and PyPI deployments.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#v009-2024-06-25","title":"v0.0.9 - 2024-06-25","text":""},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>Add decoder-only generation.</li> <li>Add tests for decoder-only generation.</li> <li>Add a notebook to showcase decoder-only generation.</li> </ul>"},{"location":"CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Restructuring the package, generate now is a subpackage istead of being a file.</li> <li>Restructure the notebooks into a folder with different notebooks for each generation architecture.</li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>Update tests to fix huggingface warnings.</li> <li>Fixed issues related to mypy in pre-commit.</li> </ul>"},{"location":"CHANGELOG/#v008-2024-05-19","title":"v0.0.8 - 2024-05-19","text":""},{"location":"CHANGELOG/#added_1","title":"Added","text":"<ul> <li>Add top-p and top-k sampling.</li> <li>Add tests for these new feature.</li> <li>update hf_generate notebook</li> </ul>"},{"location":"CHANGELOG/#changed_1","title":"Changed","text":"<ul> <li>Minor fixes and renaming.</li> </ul>"},{"location":"CHANGELOG/#v007-2024-05-8","title":"v0.0.7 - 2024-05-8","text":""},{"location":"CHANGELOG/#added_2","title":"Added","text":"<ul> <li>Add the feature of sorting samples before generation.</li> <li>Add tests for this new feature.</li> </ul>"},{"location":"CHANGELOG/#changed_2","title":"Changed","text":"<ul> <li>Minor fixes and renaming.</li> </ul>"},{"location":"CHANGELOG/#v006-2024-05-3","title":"v0.0.6 - 2024-05-3","text":""},{"location":"CHANGELOG/#added_3","title":"Added","text":"<ul> <li>Improve the code quality for beam search.</li> <li>General config updates.</li> </ul>"},{"location":"CHANGELOG/#v005-2024-04-29","title":"v0.0.5 - 2024-04-29","text":""},{"location":"CHANGELOG/#added_4","title":"Added","text":"<ul> <li>rename <code>generate_fn</code> to more appropriate name: <code>generation_forward</code></li> </ul>"},{"location":"CHANGELOG/#v004-2024-04-28","title":"v0.0.4 - 2024-04-28","text":""},{"location":"CHANGELOG/#added_5","title":"Added","text":"<ul> <li>Add multinomial sampling for both generation methods, greedy and beam search.</li> <li>Add tests for multinomial sampling.</li> <li>Inegrate some examples regarding multinomial sampling in hf_compre notebook.</li> </ul>"},{"location":"CHANGELOG/#v003-2024-04-23","title":"v0.0.3 - 2024-04-23","text":""},{"location":"CHANGELOG/#added_6","title":"Added","text":"<ul> <li>Add temperature parameter for beam search generation.</li> <li>Add tests for temperature parameter.</li> <li>Add documentation when needed.</li> </ul>"},{"location":"CHANGELOG/#changed_3","title":"Changed","text":"<ul> <li><code>length_penalty_alpha</code> parameter of beam search has been changed to <code>length_penalty</code>.</li> <li>Update the hf_compare notebook to reflect the new changes.</li> </ul>"},{"location":"CHANGELOG/#removed","title":"Removed","text":"<ul> <li>Remove the function <code>sample_tokens_probs</code> and replace its code in the <code>generate</code> method for each algorithm.</li> <li>Remove <code>minimum_penalty_tokens_length</code> from beam search generation.</li> </ul>"},{"location":"CHANGELOG/#v002-2024-04-21","title":"v0.0.2 - 2024-04-21","text":""},{"location":"CHANGELOG/#added_7","title":"Added","text":"<ul> <li>Add the beam search generation.</li> <li>Enrich the example notebook.</li> <li>Tests.</li> </ul>"},{"location":"CHANGELOG/#changed_4","title":"Changed","text":"<ul> <li>The method that samples the tokens is renamed from <code>get_next_tokens</code> to <code>sample_tokens_probs</code></li> </ul>"},{"location":"CHANGELOG/#fixed_1","title":"FIXED","text":""},{"location":"CHANGELOG/#removed_1","title":"REMOVED","text":""},{"location":"CHANGELOG/#001-2024-04-18","title":"0.0.1 - 2024-04-18","text":""},{"location":"CHANGELOG/#added_8","title":"Added","text":"<ul> <li>Generate using greedy search.</li> <li>Notebook to compare the results on a huggingface model.</li> <li>Tests.</li> </ul>"},{"location":"CHANGELOG/#removed_2","title":"Removed","text":"<ul> <li>Unused code used to initialize the project to pypi.</li> </ul>"},{"location":"CHANGELOG/#v000-2024-03-30","title":"v0.0.0 - 2024-03-30","text":""},{"location":"CHANGELOG/#added_9","title":"Added","text":"<ul> <li>v0.0.0 pushing the project to pypi</li> </ul>"},{"location":"Getting%20Started/","title":"Generate-Sequences","text":"<p><code>generate-sequences</code> is a package created to generate text from auto-regressive pytorch-based models without tears. You can think of it as huggingface generation mixin but for a pytorch model you built from scratch. No need to include it to huggingface echosystem to generate from your model. The package features greedy generation as well as beam search generation. Many sampling techniques are also supported.</p> <p><code>generate-sequences</code> can be installed with pip as follows:</p> In\u00a0[\u00a0]: Copied! <pre>pip install -U generate-sequences\n</pre> pip install -U generate-sequences <p>In encoder-decoder architecture, the typical use-case is that the model will receive encoder inputs first. These inputs will be passed as batch to the encoder in order to generate tokens from the decoder. The decoder will get the first token as the <code>decoder_start_token_id</code> then start generating untill generating the <code>eos_token_id</code> where this indicates the model is done generating with this sequence. However, it will continue generating for other sequences in the batch untill all reached the <code>eos_token_id</code> in which the generation stops.</p> <p>You can generate from an encoder-decoder architecture using greedy approach as follows. This also applies for beam search generation.</p> <p>First, Prepare your encoder inputs:</p> In\u00a0[\u00a0]: Copied! <pre># usually the sentences are enclosed with bos and eos sentences.\nencoder_sentences = [\n    '&lt;bos&gt; sentence 1 &lt;eos&gt;',\n    '&lt;bos&gt; sentence 2 &lt;eos&gt;',\n    '&lt;bos&gt; sentence 3 &lt;eos&gt;',\n    ...\n]\n# You can also handle the &lt;bos&gt; and &lt;eos&gt; in the tokenizer if you tokenizer supports that\nencoder_inputs = tokenizer.tokenize(encoder_sentences)\n</pre> # usually the sentences are enclosed with bos and eos sentences. encoder_sentences = [     ' sentence 1 ',     ' sentence 2 ',     ' sentence 3 ',     ... ] # You can also handle the  and  in the tokenizer if you tokenizer supports that encoder_inputs = tokenizer.tokenize(encoder_sentences) <p>Then, you need to tell the package how to get the logits from your model at each time step while generating. That is, you many define a method that takes the encoder and decoder inputs, and your model will generate the logits and return them. Usually, you will use the forward method of your model to get the logits, so, the recommended name of this method is <code>generation_forward</code> but you can name it literaly anything. This method can be as simple as follows:</p> In\u00a0[\u00a0]: Copied! <pre>model = MyModel(...)\n\ndef generation_forward(encoder_inputs, decoder_inputs):\n  # do something when receiving the decoder inputs at each time step\n  logits = model(encoder_inputs, decoder_inputs)\n  return logits\n</pre> model = MyModel(...)  def generation_forward(encoder_inputs, decoder_inputs):   # do something when receiving the decoder inputs at each time step   logits = model(encoder_inputs, decoder_inputs)   return logits <p>then, define the generator as follows, whether being beam search or greedy generation:</p> In\u00a0[\u00a0]: Copied! <pre>from generate_sequences import GreedyGenerator, BeamSearchGenerator\n\ngenerator = GreedyGenerator(\n    device=model.device, # make sure to have the same device as your model\n    batch_size=32, # number of samples to process at each time step\n    max_length=512, # output max length\n    generation_forward=generation_forward,\n    eos_token_id = 1, # replace this with your own\n    decoder_start_token_id=0, # replace this with your own\n)\n</pre> from generate_sequences import GreedyGenerator, BeamSearchGenerator  generator = GreedyGenerator(     device=model.device, # make sure to have the same device as your model     batch_size=32, # number of samples to process at each time step     max_length=512, # output max length     generation_forward=generation_forward,     eos_token_id = 1, # replace this with your own     decoder_start_token_id=0, # replace this with your own ) <p>Then generate:</p> In\u00a0[\u00a0]: Copied! <pre>generator.generate(encoder_inputs=encoder_inputs)\n</pre> generator.generate(encoder_inputs=encoder_inputs) <p>here is the full code in one chunk:</p> In\u00a0[\u00a0]: Copied! <pre>from generate_sequences import GreedyGenerator, BeamSearchGenerator\n\n# usually the sentences are enclosed with bos and eos sentences.\nencoder_sentences = [\n    '&lt;bos&gt; sentence 1 &lt;eos&gt;',\n    '&lt;bos&gt; sentence 2 &lt;eos&gt;',\n    '&lt;bos&gt; sentence 3 &lt;eos&gt;',\n    ...\n]\n# You can also handle the &lt;bos&gt; and &lt;eos&gt; in the tokenizer if you tokenizer supports that\nencoder_inputs = tokenizer.tokenize(encoder_sentences)\n\nmodel = MyModel(...)\n\ndef generation_forward(encoder_inputs, decoder_inputs):\n  # do something when receiving the decoder inputs at each time step\n  logits = model(encoder_inputs, decoder_inputs)\n  return logits\n\ngenerator = GreedyGenerator(\n    device=model.device, # make sure to have the same device as your model\n    batch_size=32,\n    max_length=512,\n    generation_forward=generation_forward,\n    eos_token_id = 1, # replace this with your own\n    decoder_start_token_id=0, # replace this with your own\n)\n\n# generate\ngenerator.generate(encoder_inputs=encoder_inputs)\n</pre> from generate_sequences import GreedyGenerator, BeamSearchGenerator  # usually the sentences are enclosed with bos and eos sentences. encoder_sentences = [     ' sentence 1 ',     ' sentence 2 ',     ' sentence 3 ',     ... ] # You can also handle the  and  in the tokenizer if you tokenizer supports that encoder_inputs = tokenizer.tokenize(encoder_sentences)  model = MyModel(...)  def generation_forward(encoder_inputs, decoder_inputs):   # do something when receiving the decoder inputs at each time step   logits = model(encoder_inputs, decoder_inputs)   return logits  generator = GreedyGenerator(     device=model.device, # make sure to have the same device as your model     batch_size=32,     max_length=512,     generation_forward=generation_forward,     eos_token_id = 1, # replace this with your own     decoder_start_token_id=0, # replace this with your own )  # generate generator.generate(encoder_inputs=encoder_inputs) <p>In decoder-only architecture, the typical use-case is that the model will receive the decoder inputs at each time-step in order to generate the next tokens. If you want to generate sentences from scractch, you can prompt the decoder with the decoder_start_token_id then the package will continue generating untill reaching the eos_token_id. Here is a sample example:</p> In\u00a0[\u00a0]: Copied! <pre>sentences = [\n    '&lt;bos&gt; sentence 1', # it is not expected the &lt;eos&gt; to be passed!\n    \"&lt;bos&gt;\" # you can also pass the bos only.\n]\n# You can also handle the &lt;bos&gt;  in the tokenizer if you tokenizer supports that\ndecoder_inputs = tokenizer.tokenize(sentences)\n</pre> sentences = [     ' sentence 1', # it is not expected the  to be passed!     \"\" # you can also pass the bos only. ] # You can also handle the   in the tokenizer if you tokenizer supports that decoder_inputs = tokenizer.tokenize(sentences) <p>as in the encoder-decoder architecutre, write your generation method as follows. Note that encoder_inputs are still passed but you really do not need to to anything with them.</p> In\u00a0[\u00a0]: Copied! <pre>model = MyModel(...)\n\ndef generation_forward(encoder_inputs, decoder_inputs):\n  # do something when receiving the decoder inputs at each time step\n  logits = model(decoder_inputs)\n  return logits\n</pre> model = MyModel(...)  def generation_forward(encoder_inputs, decoder_inputs):   # do something when receiving the decoder inputs at each time step   logits = model(decoder_inputs)   return logits <p>define your generator:</p> In\u00a0[\u00a0]: Copied! <pre>from generate_sequences import GreedyGenerator, BeamSearchGenerator\n\ngenerator = GreedyGenerator(\n    device=model.device, # make sure to have the same device as your model\n    batch_size=32, # number of samples to process at each time step\n    max_length=512, # output max length\n    generation_forward=generation_forward,\n    eos_token_id = 0, # replace this with your own\n)\n</pre> from generate_sequences import GreedyGenerator, BeamSearchGenerator  generator = GreedyGenerator(     device=model.device, # make sure to have the same device as your model     batch_size=32, # number of samples to process at each time step     max_length=512, # output max length     generation_forward=generation_forward,     eos_token_id = 0, # replace this with your own ) <p>then generate:</p> In\u00a0[\u00a0]: Copied! <pre>generator.generate(decoder_inputs=decoder_inputs)\n</pre> generator.generate(decoder_inputs=decoder_inputs) <p>If the inputs are a set of sentences that are not <code>&lt;bos&gt;</code> and <code>batch_size</code> is greater than 1, it is required for the inputs to be of the same shape. Pass your padding token to <code>pad_decoder_inputs</code> in the <code>generate</code> method. You can also set the padding side to <code>right</code> but THIS IS NOT a standard practice in such situation unless you know what you are doing. The typical padding side is <code>left</code> which is the default value for <code>decoder_inputs_padding_size</code> parameter.</p> <p>here is the full code in one chunk:</p> In\u00a0[\u00a0]: Copied! <pre>sentences = [\n    '&lt;bos&gt; sentence 1', # it is not expected the &lt;eos&gt; to be passed!\n    \"&lt;bos&gt;\" # you can also pass the bos only.\n]\n# You can also handle the &lt;bos&gt;  in the tokenizer if you tokenizer supports that\ndecoder_inputs = tokenizer.tokenize(sentences)\n\nmodel = MyModel(...)\n\ndef generation_forward(encoder_inputs, decoder_inputs):\n  # do something when receiving the decoder inputs at each time step\n  logits = model(decoder_inputs)\n  return logits\n\nfrom generate_sequences import GreedyGenerator, BeamSearchGenerator\n\ngenerator = GreedyGenerator(\n    device=model.device, # make sure to have the same device as your model\n    batch_size=32, # number of samples to process at each time step\n    max_length=512, # output max length\n    generation_forward=generation_forward,\n    eos_token_id = 1, # replace this with your own\n    decoder_start_token_id=0, # replace this with your own\n)\n\n# generate\ngenerator.generate(decoder_inputs=decoder_inputs)\n</pre> sentences = [     ' sentence 1', # it is not expected the  to be passed!     \"\" # you can also pass the bos only. ] # You can also handle the   in the tokenizer if you tokenizer supports that decoder_inputs = tokenizer.tokenize(sentences)  model = MyModel(...)  def generation_forward(encoder_inputs, decoder_inputs):   # do something when receiving the decoder inputs at each time step   logits = model(decoder_inputs)   return logits  from generate_sequences import GreedyGenerator, BeamSearchGenerator  generator = GreedyGenerator(     device=model.device, # make sure to have the same device as your model     batch_size=32, # number of samples to process at each time step     max_length=512, # output max length     generation_forward=generation_forward,     eos_token_id = 1, # replace this with your own     decoder_start_token_id=0, # replace this with your own )  # generate generator.generate(decoder_inputs=decoder_inputs) <p>Below are some useful parameters to be passed to the generator. These parameters can be used regardless of the generation method used.</p> <p><code>generate-sequences</code> support various sampling methods. To get an idea on sampling, I strongly advice to read this great article (https://huggingface.co/blog/how-to-generate) for more details on generation methods. Consider going over the following points for an overview:</p> <ul> <li>Setting <code>multinomial_sampling=True</code> will generate tokens based on multinomial distribution instead of the default greedy approach.</li> <li>You can play with the <code>temperature</code> by passing a value between 0,1.</li> <li><code>top_k_sampling</code> and <code>top_p_sampling</code> are also supported.</li> </ul> <p>Usually, inputs comes with various lengths. However, this is ineffecient as the padding will always consider the largest sample in the batch. If samples are ordered, then largest samples will be in teh begining, taking more time at the beging and utilizing the padding effectively. As the generation paces over batches, it moves faster. This parameter is <code>True</code> by default. Usually, you do not want to set it to <code>False</code> unless you know what you are doing.</p> <p>By setting this parameter to True in the generator, two lists will be returned. The first list is the output ids. The second list is a list of tuples where each tuple is the output token id along with its logit value. Logits are useful for many usecases like calculating perplexity, for instance.</p>"},{"location":"Getting%20Started/#generate-sequences","title":"Generate-Sequences\u00b6","text":""},{"location":"Getting%20Started/#installation","title":"Installation\u00b6","text":""},{"location":"Getting%20Started/#encoder-decoder-architecutres","title":"encoder-decoder architecutres\u00b6","text":""},{"location":"Getting%20Started/#decoder-only-architectures","title":"decoder-only architectures\u00b6","text":""},{"location":"Getting%20Started/#additional-parameters","title":"Additional parameters\u00b6","text":""},{"location":"Getting%20Started/#sampling","title":"Sampling\u00b6","text":""},{"location":"Getting%20Started/#sort_inputs_by_size","title":"<code>sort_inputs_by_size</code>\u00b6","text":""},{"location":"Getting%20Started/#return_logits","title":"<code>return_logits</code>\u00b6","text":""},{"location":"examples/hugginface_decoder_only/","title":"Decoder-Only Architecture","text":"<p>In this notebook, we are going to run the <code>.generate</code> method of huggingface as well as the generation process of <code>generate-sequences</code>. The architecture we are going to run on is a decoder-only architecture, a GPT-like architecture.</p> In\u00a0[1]: Copied! <pre>import torch\nfrom tqdm.auto import tqdm\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom generate_sequences import GreedyGenerator\n</pre> import torch from tqdm.auto import tqdm from transformers import GPT2LMHeadModel, GPT2Tokenizer from generate_sequences import GreedyGenerator In\u00a0[2]: Copied! <pre>device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\nmodel_name = \"gpt2\"  # You can choose other variants like 'gpt2-medium', 'gpt2-large', 'gpt2-xl'\nmodel = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\ntokenizer.pad_token=tokenizer.decode(model.generation_config.bos_token_id)\ntokenizer.padding_side = 'left'\n</pre> device = 'cuda' if torch.cuda.is_available() else 'cpu'   model_name = \"gpt2\"  # You can choose other variants like 'gpt2-medium', 'gpt2-large', 'gpt2-xl' model = GPT2LMHeadModel.from_pretrained(model_name).to(device)  tokenizer = GPT2Tokenizer.from_pretrained(model_name) tokenizer.pad_token=tokenizer.decode(model.generation_config.bos_token_id) tokenizer.padding_side = 'left' <pre>/home/majed_alshaibani/.virtualenvs/generate-sequences/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n</pre> In\u00a0[3]: Copied! <pre># prompts to generate\ninput_texts = [\n    \"Once upon a time\",\n    \"The quick brown fox\",\n    \"Last night I dreamed\",\n    \"In the heart of the city\",\n    \"At the edge of the world\",\n]\nMAX_LENGTH = 50\nBATCH_SIZE = 2\n</pre> # prompts to generate input_texts = [     \"Once upon a time\",     \"The quick brown fox\",     \"Last night I dreamed\",     \"In the heart of the city\",     \"At the edge of the world\", ] MAX_LENGTH = 50 BATCH_SIZE = 2 In\u00a0[4]: Copied! <pre>def get_batches(texts, batch_size):\n    \"\"\"Yield successive n-sized batches from texts.\"\"\"\n    for i in range(0, len(texts), batch_size):\n        yield texts[i:i + batch_size]\n</pre> def get_batches(texts, batch_size):     \"\"\"Yield successive n-sized batches from texts.\"\"\"     for i in range(0, len(texts), batch_size):         yield texts[i:i + batch_size] In\u00a0[5]: Copied! <pre>generated_texts = []\nfor batch in tqdm(get_batches(input_texts, BATCH_SIZE), desc=\"Generating Texts\"):\n    # Tokenize batch\n    encoded_input = tokenizer(\n        batch,\n        padding=True,\n        return_tensors=\"pt\",\n    ).to(device)\n\n    # Generate text\n    output = model.generate(\n        input_ids=encoded_input[\"input_ids\"],\n        attention_mask=encoded_input[\"attention_mask\"],\n        max_length=MAX_LENGTH,  # Max length of the generated text\n    )\n\n    # Decode generated texts\n    batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n    generated_texts.extend(batch_generated_texts)\n\n# Print all collected results\nfor input_text, generated_text in zip(input_texts, generated_texts):\n    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")\n</pre> generated_texts = [] for batch in tqdm(get_batches(input_texts, BATCH_SIZE), desc=\"Generating Texts\"):     # Tokenize batch     encoded_input = tokenizer(         batch,         padding=True,         return_tensors=\"pt\",     ).to(device)      # Generate text     output = model.generate(         input_ids=encoded_input[\"input_ids\"],         attention_mask=encoded_input[\"attention_mask\"],         max_length=MAX_LENGTH,  # Max length of the generated text     )      # Decode generated texts     batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]     generated_texts.extend(batch_generated_texts)  # Print all collected results for input_text, generated_text in zip(input_texts, generated_texts):     print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\") <pre>Generating Texts: 0it [00:00, ?it/s]</pre> <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n</pre> <pre>Input: Once upon a time\nGenerated: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n\nInput: The quick brown fox\nGenerated: The quick brown foxes are a great way to get a little bit of a kick out of your dog.\n\nThe quick brown foxes are a great way to get a little bit of a kick out of your dog. The quick brown fox\n\nInput: Last night I dreamed\nGenerated: Last night I dreamed of a day when I could go to the beach and swim with my friends. I was so excited to be back in the ocean. I was so excited to be back in the ocean. I was so excited to be\n\nInput: In the heart of the city\nGenerated: In the heart of the city, the city of San Francisco is a city of people. It's a place where people come together to celebrate, to celebrate, to celebrate. It's a place where people come together to celebrate, to celebrate, to\n\nInput: At the edge of the world\nGenerated: At the edge of the world, the world is a place of great beauty. The world is a place of great fear. The world is a place of great fear. The world is a place of great fear. The world is a place of great\n\n</pre> In\u00a0[6]: Copied! <pre>generated_texts = []\nfor batch in tqdm(get_batches(input_texts, BATCH_SIZE), desc=\"Generating Texts\"):\n    # Tokenize batch\n    encoded_input = tokenizer(batch, return_tensors=\"pt\", padding=True).to(device)\n    \n    # Generate text\n    output = model.generate(\n        input_ids=encoded_input[\"input_ids\"],\n        attention_mask=encoded_input[\"attention_mask\"],\n        max_length=MAX_LENGTH,  # Max length of the generated text\n        top_k=50,  # Limits the sampling pool to the top_k tokens\n        top_p=0.95,  # Nucleus sampling: sample only from top_p probability mass\n        temperature=0.7,  # Sampling temperature: lower value -&gt; more conservative, higher value -&gt; more random\n        do_sample=True  # Enable sampling\n    )\n    \n    # Decode generated texts\n    batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n    generated_texts.extend(batch_generated_texts)\n    \n# Print all collected results\nfor input_text, generated_text in zip(input_texts, generated_texts):\n    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")\n</pre> generated_texts = [] for batch in tqdm(get_batches(input_texts, BATCH_SIZE), desc=\"Generating Texts\"):     # Tokenize batch     encoded_input = tokenizer(batch, return_tensors=\"pt\", padding=True).to(device)          # Generate text     output = model.generate(         input_ids=encoded_input[\"input_ids\"],         attention_mask=encoded_input[\"attention_mask\"],         max_length=MAX_LENGTH,  # Max length of the generated text         top_k=50,  # Limits the sampling pool to the top_k tokens         top_p=0.95,  # Nucleus sampling: sample only from top_p probability mass         temperature=0.7,  # Sampling temperature: lower value -&gt; more conservative, higher value -&gt; more random         do_sample=True  # Enable sampling     )          # Decode generated texts     batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]     generated_texts.extend(batch_generated_texts)      # Print all collected results for input_text, generated_text in zip(input_texts, generated_texts):     print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\") <pre>Generating Texts: 0it [00:00, ?it/s]</pre> <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n</pre> <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n</pre> <pre>Input: Once upon a time\nGenerated: Once upon a time, you might have heard about the \"Halloween Horror\" phenomenon. The Halloween Horror is a Halloween film that was made for a Halloween convention in New York City and screened at the 2015 Halloween Horror Film Festival. The film had a\n\nInput: The quick brown fox\nGenerated: The quick brown fox is a bit more challenging as the fox can't even be seen unless you're looking closely. The fox also has a tendency to go straight at you, and it's more difficult to get your eyes on the fox if you're\n\nInput: Last night I dreamed\nGenerated: Last night I dreamed about being the first person to actually see what it was like to be in a place like this. It was amazing. I was so honored to be able to be a part of it. I really feel like I'm\n\nInput: In the heart of the city\nGenerated: In the heart of the city is the Church of the Holy Trinity. The Trinity is the living God and Father of all things, the Savior of the world, the Creator and Ruler of all things.\n\nIn the Bible, God is the Father\n\nInput: At the edge of the world\nGenerated: At the edge of the world, the men of my village would come to my tent to meet me at the door.\n\nI said nothing.\n\nThe men of my village were the men of the city. I said nothing.\n\n\n\n</pre> In\u00a0[7]: Copied! <pre>def generation_forward(encoder_inputs, decoder_inputs):\n    return model(input_ids=decoder_inputs).logits\n</pre> def generation_forward(encoder_inputs, decoder_inputs):     return model(input_ids=decoder_inputs).logits In\u00a0[8]: Copied! <pre>gpt2_greedy_generator = GreedyGenerator(\n    use_tqdm=True,\n    batch_size=BATCH_SIZE,\n    max_length=MAX_LENGTH,\n    device=model.device,\n    generation_forward=generation_forward,\n    eos_token_id=model.generation_config.eos_token_id,\n    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n)\n</pre> gpt2_greedy_generator = GreedyGenerator(     use_tqdm=True,     batch_size=BATCH_SIZE,     max_length=MAX_LENGTH,     device=model.device,     generation_forward=generation_forward,     eos_token_id=model.generation_config.eos_token_id,     decoder_start_token_id=model.generation_config.decoder_start_token_id, ) In\u00a0[9]: Copied! <pre>generated_texts = []\nfor batch in get_batches(input_texts, BATCH_SIZE):\n    # Tokenize batch\n    encoded_input = tokenizer(batch, return_tensors=\"pt\", padding=True).to(device)\n    # Generate text\n    output = gpt2_greedy_generator.generate(\n        encoder_inputs=None,\n        decoder_inputs=encoded_input[\"input_ids\"],\n        pad_decoder_inputs=tokenizer.bos_token_id,\n    )\n    \n    # Decode generated texts\n    batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n    generated_texts.extend(batch_generated_texts)\n    \n# Print all collected results\nfor input_text, generated_text in zip(input_texts, generated_texts):\n    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")\n</pre> generated_texts = [] for batch in get_batches(input_texts, BATCH_SIZE):     # Tokenize batch     encoded_input = tokenizer(batch, return_tensors=\"pt\", padding=True).to(device)     # Generate text     output = gpt2_greedy_generator.generate(         encoder_inputs=None,         decoder_inputs=encoded_input[\"input_ids\"],         pad_decoder_inputs=tokenizer.bos_token_id,     )          # Decode generated texts     batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]     generated_texts.extend(batch_generated_texts)      # Print all collected results for input_text, generated_text in zip(input_texts, generated_texts):     print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\") <pre>Generating Sequences:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>Generating Sequences:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>Generating Sequences: 0it [00:00, ?it/s]</pre> <pre>Input: Once upon a time\nGenerated: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n\nInput: The quick brown fox\nGenerated: The quick brown foxes are a great way to get a little bit of a kick out of your dog.\n\nThe quick brown foxes are a great way to get a little bit of a kick out of your dog. The quick brown fox\n\nInput: Last night I dreamed\nGenerated: Last night I dreamed of a day when I could go to the beach and swim with my friends. I was so excited to see the ocean, the waves, the waves. I was so excited to see the ocean, the waves, the\n\nInput: In the heart of the city\nGenerated: In the heart of the city, the city of San Francisco is a city of people. It's a place where people come together to celebrate, to celebrate, to celebrate. It's a place where people come together to celebrate, to celebrate, to\n\nInput: At the edge of the world\nGenerated: At the edge of the world, the world is a place of great beauty. The world is a place of great fear. The world is a place of great fear. The world is a place of great fear. The world is a place of great\n\n</pre> In\u00a0[10]: Copied! <pre>def generation_forward(encoder_inputs, decoder_inputs):\n    return model(input_ids=decoder_inputs).logits\n</pre> def generation_forward(encoder_inputs, decoder_inputs):     return model(input_ids=decoder_inputs).logits In\u00a0[11]: Copied! <pre>gpt2_greedy_generator = GreedyGenerator(\n    use_tqdm=True,\n    top_k_sampling=50,\n    top_p_sampling=0.95,\n    device=model.device,\n    batch_size=BATCH_SIZE,\n    max_length=MAX_LENGTH,\n    multinomial_sampling=True,\n    generation_forward=generation_forward,\n    eos_token_id=model.generation_config.eos_token_id,\n    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n)\n</pre> gpt2_greedy_generator = GreedyGenerator(     use_tqdm=True,     top_k_sampling=50,     top_p_sampling=0.95,     device=model.device,     batch_size=BATCH_SIZE,     max_length=MAX_LENGTH,     multinomial_sampling=True,     generation_forward=generation_forward,     eos_token_id=model.generation_config.eos_token_id,     decoder_start_token_id=model.generation_config.decoder_start_token_id, ) In\u00a0[12]: Copied! <pre>generated_texts = []\nfor batch in get_batches(input_texts, BATCH_SIZE):\n    # Tokenize batch\n    encoded_input = tokenizer(batch, return_tensors=\"pt\", padding=True).to(device)\n    # Generate text\n    output = gpt2_greedy_generator.generate(\n        encoder_inputs=None,\n        decoder_inputs=encoded_input[\"input_ids\"],\n        pad_decoder_inputs=tokenizer.bos_token_id,\n    )\n    \n    # Decode generated texts\n    batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n    generated_texts.extend(batch_generated_texts)\n    \n# Print all collected results\nfor input_text, generated_text in zip(input_texts, generated_texts):\n    print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\")\n</pre> generated_texts = [] for batch in get_batches(input_texts, BATCH_SIZE):     # Tokenize batch     encoded_input = tokenizer(batch, return_tensors=\"pt\", padding=True).to(device)     # Generate text     output = gpt2_greedy_generator.generate(         encoder_inputs=None,         decoder_inputs=encoded_input[\"input_ids\"],         pad_decoder_inputs=tokenizer.bos_token_id,     )          # Decode generated texts     batch_generated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in output]     generated_texts.extend(batch_generated_texts)      # Print all collected results for input_text, generated_text in zip(input_texts, generated_texts):     print(f\"Input: {input_text}\\nGenerated: {generated_text}\\n\") <pre>Generating Sequences:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>Generating Sequences:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>Generating Sequences: 0it [00:00, ?it/s]</pre> <pre>Input: Once upon a time\nGenerated: Once upon a time when every thought and emotion of the human mind was to be consumed by the same thought and emotion, we are confronted with a false and utterly ungrateful reality. Our ignorance is the only thing that can bring about the correct mental\n\nInput: The quick brown fox\nGenerated: The quick brown fox out of the corner of my eye and I realised I'd found my spot on this list for the best price. My sister and I, our only child, just had started school in the summer of 2015 so we'd not been\n\nInput: Last night I dreamed\nGenerated: Last night I dreamed about how beautiful and beautiful this summer's beautiful people were.\n\nThe day after the premiere of my new book 'The End of History', I was having dinner in the park at the time of the premiere at the\n\nInput: In the heart of the city\nGenerated: In the heart of the city, two-thirds of Chicago's schools don't have a superintendent.\n\nWhile the city offers some flexibility in terms of whether or not district leaders can appoint schools superintendent, the mayor says he is not making public education\n\nInput: At the edge of the world\nGenerated: At the edge of the world, he was the first person on Earth who took on more energy. His heart didn't want anything to do with it. His body was empty as his body had been built in the beginning. It's because he has\n\n</pre>"},{"location":"examples/hugginface_decoder_only/#decoder-only-architecture","title":"Decoder-Only Architecture\u00b6","text":""},{"location":"examples/hugginface_decoder_only/#load-the-model-and-dataset","title":"Load the Model and Dataset\u00b6","text":""},{"location":"examples/hugginface_decoder_only/#preparation","title":"Preparation\u00b6","text":""},{"location":"examples/hugginface_decoder_only/#generate-text-using-huggingface-generate-method","title":"Generate text using HuggingFace <code>generate</code> method\u00b6","text":""},{"location":"examples/hugginface_decoder_only/#generate-with-sampling","title":"Generate with sampling\u00b6","text":""},{"location":"examples/hugginface_decoder_only/#generate-with-generate-sequences-greedy-generation","title":"Generate with generate-sequences, greedy generation\u00b6","text":""},{"location":"examples/hugginface_decoder_only/#generate-with-generate-sequences-greedy-with-sampling","title":"Generate with generate-sequences, greedy with sampling\u00b6","text":""},{"location":"examples/huggingface_encoder_decoder/","title":"Encoder-Decoder Architecture","text":"<p>In this notebook, we are going to run the <code>.generate</code> method of huggingface as well as the generation process of <code>generate-sequences</code> on a sequence-to-sequence pretrained pytorch machine translation model. The architecture we are going to run on is a encoder-decoder architecture, a sequence-sequence-based architecture.</p> In\u00a0[1]: Copied! <pre>import json\n\nimport torch\n\nimport evaluate\nimport datasets\n\nfrom tqdm.auto import tqdm\n\nfrom transformers import MarianTokenizer, MarianMTModel\n\nfrom generate_sequences import GreedyGenerator, BeamSearchGenerator\n</pre> import json  import torch  import evaluate import datasets  from tqdm.auto import tqdm  from transformers import MarianTokenizer, MarianMTModel  from generate_sequences import GreedyGenerator, BeamSearchGenerator In\u00a0[2]: Copied! <pre># load the translation model from transformers\n# model_name = \"Helsinki-NLP/opus-mt-ar-en\"\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\nmodel_name = \"marefa-nlp/marefa-mt-en-ar\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\n\n\nbleu_scorer = evaluate.load(\"sacrebleu\")\n\ntest_dataset = datasets.load_dataset('iwslt2017','iwslt2017-ar-en', split='test')\n</pre> # load the translation model from transformers # model_name = \"Helsinki-NLP/opus-mt-ar-en\" # tokenizer = AutoTokenizer.from_pretrained(model_name) # model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')  device = 'cuda' if torch.cuda.is_available() else 'cpu'   model_name = \"marefa-nlp/marefa-mt-en-ar\" tokenizer = MarianTokenizer.from_pretrained(model_name) model = MarianMTModel.from_pretrained(model_name)   bleu_scorer = evaluate.load(\"sacrebleu\")  test_dataset = datasets.load_dataset('iwslt2017','iwslt2017-ar-en', split='test') <pre>/home/magedsaeed/virtualenvs/generate-sequences/lib/python3.12/site-packages/datasets/load.py:1486: FutureWarning: The repository for iwslt2017 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/iwslt2017\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n</pre> In\u00a0[3]: Copied! <pre>source_language = 'en'\ntarget_language = 'ar'\n</pre> source_language = 'en' target_language = 'ar' <p>testing on 10 samples only.</p> In\u00a0[4]: Copied! <pre>input_texts = [example[source_language] for example in test_dataset['translation']][-10:]\ntargets = [example[target_language] for example in test_dataset['translation']][-10:]\nlen(input_texts), len(targets), input_texts[:5], targets[:5]\n</pre> input_texts = [example[source_language] for example in test_dataset['translation']][-10:] targets = [example[target_language] for example in test_dataset['translation']][-10:] len(input_texts), len(targets), input_texts[:5], targets[:5] Out[4]: <pre>(10,\n 10,\n [\"One major consequence of this work  is that maybe all of these decades,  we've had the whole concept of cybernetic revolt  in reverse.\",\n  \"It's not that machines first become intelligent  and then megalomaniacal  and try to take over the world.\",\n  \"It's quite the opposite,  that the urge to take control  of all possible futures  is a more fundamental principle  than that of intelligence,  that general intelligence may in fact emerge  directly from this sort of control-grabbing,  rather than vice versa.\",\n  'Another important consequence is goal seeking.',\n  \"I'm often asked, how does the ability to seek goals  follow from this sort of framework?\"],\n ['\u0623\u062d\u062f \u0627\u0644\u0639\u0648\u0627\u0642\u0628 \u0627\u0644\u0643\u0628\u0631\u0649 \u0644\u0647\u0630\u0627 \u0627\u0644\u0639\u0645\u0644 \u0647\u0648 \u0623\u0646\u0647 \u0644\u0631\u0628\u0645\u0627 \u0637\u0648\u0627\u0644 \u0643\u0644 \u0647\u0630\u0647 \u0627\u0644\u0639\u0642\u0648\u062f\u060c \u0643\u0627\u0646 \u0644\u062f\u064a\u0646\u0627 \u0627\u0644\u0645\u0641\u0647\u0648\u0645 \u0627\u0644\u0639\u0643\u0633\u064a \u0644\u0644\u062b\u0648\u0631\u0629 \u0627\u0644\u0622\u0644\u064a\u0629.',\n  '\u0627\u0644\u0623\u0645\u0631 \u0644\u064a\u0633 \u0641\u064a \u0623\u0646 \u0627\u0644\u0622\u0644\u0627\u062a \u062a\u0635\u0628\u062d \u0630\u0643\u064a\u0629 \u0641\u064a \u0627\u0644\u0628\u062f\u0627\u064a\u0629 \u062b\u0645 \u064a\u0646\u062a\u0627\u0628\u0647\u0627 \u062c\u0646\u0648\u0646 \u0627\u0644\u0639\u0638\u0645\u0629 \u0648 \u062a\u062d\u0627\u0648\u0644 \u0627\u0644\u0633\u064a\u0637\u0631\u0629 \u0639\u0644\u0649 \u0627\u0644\u0639\u0627\u0644\u0645.',\n  '\u0625\u0646\u0647 \u062a\u0645\u0627\u0645\u0627\u064b \u0627\u0644\u0639\u0643\u0633\u060c \u0623\u0646 \u0627\u0644\u0646\u0632\u0639\u0629 \u0644\u0644\u0633\u064a\u0637\u0631\u0629 \u0639\u0644\u0649 \u0643\u0644 \u0627\u0644\u0623\u0632\u0645\u0646\u0629 \u0627\u0644\u0645\u0633\u062a\u0642\u0628\u0644\u064a\u0629 \u0627\u0644\u0648\u0627\u0631\u062f\u0629 \u0647\u064a \u0645\u0628\u062f\u0623 \u0623\u0633\u0627\u0633\u064a \u0623\u0643\u062b\u0631 \u0645\u0646 \u0645\u0628\u062f\u0623 \u0627\u0644\u0630\u0643\u0627\u0621\u060c \u0623\u0646 \u0646\u0648\u0627\u062d\u064a \u0627\u0644\u0630\u0643\u0627\u0621 \u0627\u0644\u0639\u0627\u0645\u0629 \u064a\u0645\u0643\u0646 \u0641\u064a \u0627\u0644\u062d\u0642\u064a\u0642\u0629 \u0623\u0646 \u062a\u0646\u0628\u0639\u062b \u0645\u0628\u0627\u0634\u0631\u0629 \u0645\u0646 \u0627\u0644\u0633\u064a\u0637\u0631\u0629\u060c \u0628\u062f\u0644\u0627\u064b \u0645\u0646 \u0623\u0646 \u064a\u0643\u0648\u0646 \u0627\u0644\u0623\u0645\u0631 \u0628\u0627\u0644\u0639\u0643\u0633.',\n  '\u0639\u0627\u0642\u0628\u0629 \u0623\u062e\u0631\u0649 \u0645\u0647\u0645\u0629 \u0647\u064a \u0627\u0644\u0628\u062d\u062b \u0639\u0646 \u0627\u0644\u0647\u062f\u0641.',\n  '\u0625\u0646\u0646\u064a \u0623\u064f\u0633\u0623\u0644 \u063a\u0627\u0644\u0628\u0627\u064b\u060c \u0643\u064a\u0641 \u064a\u0645\u0643\u0646 \u062a\u0641\u0633\u064a\u0631 \u0642\u062f\u0631\u0629 \u0627\u0644\u0628\u062d\u062b \u0639\u0646 \u0627\u0644\u0623\u0647\u062f\u0627\u0641 \u0641\u064a \u0647\u0630\u0627 \u0627\u0644\u0625\u0637\u0627\u0631\u061f'])</pre> <p>setting <code>use_cache=False</code> as this disables optimizations being applied to transformers architecture [https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig.use_cache].</p> In\u00a0[5]: Copied! <pre>model.generation_config.num_beams=1\nmodel.generation_config.use_cache = False\nmodel.generation_config.batch_size=2\n</pre> model.generation_config.num_beams=1 model.generation_config.use_cache = False model.generation_config.batch_size=2 In\u00a0[6]: Copied! <pre>def get_batches(inputs,batch_size):\n    for i in tqdm(\n            range(0, len(inputs), batch_size),\n            desc=\"Generating Sequences\",\n            total=len(inputs) // batch_size,\n        ):\n        yield inputs[i : i + batch_size]\n</pre> def get_batches(inputs,batch_size):     for i in tqdm(             range(0, len(inputs), batch_size),             desc=\"Generating Sequences\",             total=len(inputs) // batch_size,         ):         yield inputs[i : i + batch_size] <p>setting <code>do_sample=False</code>.</p> In\u00a0[7]: Copied! <pre>def translate(texts):\n    translated_texts = list()\n    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n        translated_tokens = model.generate(\n            do_sample=False,\n            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n        )\n        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n    return translated_texts\n</pre> def translate(texts):     translated_texts = list()     for batch in get_batches(texts,batch_size=model.generation_config.batch_size):         translated_tokens = model.generate(             do_sample=False,             **tokenizer(batch, return_tensors=\"pt\",padding=True),         )         translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]     return translated_texts In\u00a0[8]: Copied! <pre># Example batch of input sentences\nhf_predictions = translate(input_texts)\nlen(input_texts), len(hf_predictions), len(targets)\n</pre> # Example batch of input sentences hf_predictions = translate(input_texts) len(input_texts), len(hf_predictions), len(targets) <pre>Generating Sequences:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> Out[8]: <pre>(10, 10, 10)</pre> In\u00a0[9]: Copied! <pre>bleu_scorer.compute(predictions=hf_predictions, references=targets)\n</pre> bleu_scorer.compute(predictions=hf_predictions, references=targets) Out[9]: <pre>{'score': 15.796125110909543,\n 'counts': [128, 58, 28, 13],\n 'totals': [264, 254, 244, 235],\n 'precisions': [48.484848484848484,\n  22.834645669291337,\n  11.475409836065573,\n  5.531914893617022],\n 'bp': 0.9701515036966302,\n 'sys_len': 264,\n 'ref_len': 272}</pre> In\u00a0[10]: Copied! <pre>def translate(texts):\n    translated_texts = list()\n    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n        translated_tokens = model.generate(\n            top_k=100,\n            top_p=0.8,\n            do_sample=True,\n            temperature=0.9,\n            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n        )\n        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n    return translated_texts\n</pre> def translate(texts):     translated_texts = list()     for batch in get_batches(texts,batch_size=model.generation_config.batch_size):         translated_tokens = model.generate(             top_k=100,             top_p=0.8,             do_sample=True,             temperature=0.9,             **tokenizer(batch, return_tensors=\"pt\",padding=True),         )         translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]     return translated_texts In\u00a0[11]: Copied! <pre># Example batch of input sentences\nhf_predictions = translate(input_texts)\nlen(input_texts), len(hf_predictions), len(targets)\n</pre> # Example batch of input sentences hf_predictions = translate(input_texts) len(input_texts), len(hf_predictions), len(targets) <pre>Generating Sequences:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> Out[11]: <pre>(10, 10, 10)</pre> In\u00a0[12]: Copied! <pre>bleu_scorer.compute(predictions=hf_predictions, references=targets)\n</pre> bleu_scorer.compute(predictions=hf_predictions, references=targets) Out[12]: <pre>{'score': 15.895432255918095,\n 'counts': [124, 56, 31, 13],\n 'totals': [267, 257, 248, 239],\n 'precisions': [46.441947565543074,\n  21.78988326848249,\n  12.5,\n  5.439330543933054],\n 'bp': 0.9814476614410015,\n 'sys_len': 267,\n 'ref_len': 272}</pre> <p>We set explicitly <code>do_sample=False</code>.</p> In\u00a0[13]: Copied! <pre>def translate(texts):\n    translated_texts = list()\n    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n        translated_tokens = model.generate(\n            num_beams=4,\n            do_sample=False,\n            length_penalty=0.6,\n            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n        )\n        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n    return translated_texts\n</pre> def translate(texts):     translated_texts = list()     for batch in get_batches(texts,batch_size=model.generation_config.batch_size):         translated_tokens = model.generate(             num_beams=4,             do_sample=False,             length_penalty=0.6,             **tokenizer(batch, return_tensors=\"pt\",padding=True),         )         translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]     return translated_texts In\u00a0[14]: Copied! <pre># Example batch of input sentences\nbeam_search_hf_predictions = translate(input_texts)\nlen(input_texts), len(beam_search_hf_predictions), len(targets)\n</pre> # Example batch of input sentences beam_search_hf_predictions = translate(input_texts) len(input_texts), len(beam_search_hf_predictions), len(targets) <pre>Generating Sequences:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> Out[14]: <pre>(10, 10, 10)</pre> In\u00a0[15]: Copied! <pre>bleu_scorer.compute(predictions=beam_search_hf_predictions, references=targets)\n</pre> bleu_scorer.compute(predictions=beam_search_hf_predictions, references=targets) Out[15]: <pre>{'score': 20.084845774979332,\n 'counts': [134, 66, 38, 21],\n 'totals': [262, 252, 242, 233],\n 'precisions': [51.14503816793893,\n  26.19047619047619,\n  15.702479338842975,\n  9.012875536480687],\n 'bp': 0.9625512774839297,\n 'sys_len': 262,\n 'ref_len': 272}</pre> In\u00a0[16]: Copied! <pre>def translate(texts):\n    translated_texts = list()\n    for batch in get_batches(texts,batch_size=model.generation_config.batch_size):\n        translated_tokens = model.generate(\n            top_k=100,\n            top_p=0.8,\n            num_beams=4,\n            do_sample=True,\n            temperature=0.9,\n            length_penalty=0.6,\n            **tokenizer(batch, return_tensors=\"pt\",padding=True),\n        )\n        translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n    return translated_texts\n</pre> def translate(texts):     translated_texts = list()     for batch in get_batches(texts,batch_size=model.generation_config.batch_size):         translated_tokens = model.generate(             top_k=100,             top_p=0.8,             num_beams=4,             do_sample=True,             temperature=0.9,             length_penalty=0.6,             **tokenizer(batch, return_tensors=\"pt\",padding=True),         )         translated_texts += [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]     return translated_texts In\u00a0[17]: Copied! <pre># Example batch of input sentences\nbeam_search_hf_predictions = translate(input_texts)\nlen(input_texts), len(beam_search_hf_predictions), len(targets)\n</pre> # Example batch of input sentences beam_search_hf_predictions = translate(input_texts) len(input_texts), len(beam_search_hf_predictions), len(targets) <pre>Generating Sequences:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> Out[17]: <pre>(10, 10, 10)</pre> In\u00a0[18]: Copied! <pre>bleu_scorer.compute(predictions=beam_search_hf_predictions, references=targets)\n</pre> bleu_scorer.compute(predictions=beam_search_hf_predictions, references=targets) Out[18]: <pre>{'score': 20.198004312748168,\n 'counts': [135, 67, 38, 21],\n 'totals': [262, 252, 242, 233],\n 'precisions': [51.52671755725191,\n  26.58730158730159,\n  15.702479338842975,\n  9.012875536480687],\n 'bp': 0.9625512774839297,\n 'sys_len': 262,\n 'ref_len': 272}</pre> <p>checking model config</p> In\u00a0[19]: Copied! <pre>model.generation_config\n</pre> model.generation_config Out[19]: <pre>GenerationConfig {\n  \"bad_words_ids\": [\n    [\n      62801\n    ]\n  ],\n  \"batch_size\": 2,\n  \"bos_token_id\": 0,\n  \"decoder_start_token_id\": 62801,\n  \"eos_token_id\": 0,\n  \"forced_eos_token_id\": 0,\n  \"max_length\": 512,\n  \"pad_token_id\": 62801,\n  \"use_cache\": false\n}</pre> <p>This is the generation function that is used for both, greedy and beam search generation</p> In\u00a0[20]: Copied! <pre>encoder_outputs = {}\n\n\ndef generate(inputs, decoder_input_ids):\n    global encoder_outputs\n    tokenizer_results = tokenizer(\n        inputs,\n        return_tensors=\"pt\",\n        padding=True,\n    )\n    if not encoder_outputs.get(json.dumps(inputs)):\n        input_ids, attention_mask = (\n            tokenizer_results[\"input_ids\"],\n            tokenizer_results[\"attention_mask\"],\n        )\n        encoder_outputs[json.dumps(inputs)] = model.get_encoder()(\n            input_ids.repeat_interleave(\n                model.generation_config.num_beams,\n                dim=0,\n            ),\n            return_dict=True,\n            attention_mask=attention_mask,\n        )\n    model_outputs = model(\n        **tokenizer_results,\n        decoder_input_ids=decoder_input_ids,\n        encoder_outputs=encoder_outputs[json.dumps(inputs)],\n    )\n    return model_outputs.logits\n</pre> encoder_outputs = {}   def generate(inputs, decoder_input_ids):     global encoder_outputs     tokenizer_results = tokenizer(         inputs,         return_tensors=\"pt\",         padding=True,     )     if not encoder_outputs.get(json.dumps(inputs)):         input_ids, attention_mask = (             tokenizer_results[\"input_ids\"],             tokenizer_results[\"attention_mask\"],         )         encoder_outputs[json.dumps(inputs)] = model.get_encoder()(             input_ids.repeat_interleave(                 model.generation_config.num_beams,                 dim=0,             ),             return_dict=True,             attention_mask=attention_mask,         )     model_outputs = model(         **tokenizer_results,         decoder_input_ids=decoder_input_ids,         encoder_outputs=encoder_outputs[json.dumps(inputs)],     )     return model_outputs.logits In\u00a0[21]: Copied! <pre>greedy_sequences_generator = GreedyGenerator(\n    use_tqdm=True,\n    sort_inputs_by_size=True,\n    device=model.device,\n    generation_forward=generate,\n    batch_size=model.generation_config.batch_size,\n    max_length=model.generation_config.max_length,\n    eos_token_id=model.generation_config.eos_token_id,\n    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n)\n</pre> greedy_sequences_generator = GreedyGenerator(     use_tqdm=True,     sort_inputs_by_size=True,     device=model.device,     generation_forward=generate,     batch_size=model.generation_config.batch_size,     max_length=model.generation_config.max_length,     eos_token_id=model.generation_config.eos_token_id,     decoder_start_token_id=model.generation_config.decoder_start_token_id, ) In\u00a0[22]: Copied! <pre>prediction_ids = greedy_sequences_generator.generate(input_texts)\npredictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\nlen(input_texts), len(predictions), len(targets)\n</pre> prediction_ids = greedy_sequences_generator.generate(input_texts) predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True) len(input_texts), len(predictions), len(targets) <pre>Generating Sequences:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> Out[22]: <pre>(10, 10, 10)</pre> In\u00a0[23]: Copied! <pre>bleu_scorer.compute(predictions=predictions, references=targets)\n</pre> bleu_scorer.compute(predictions=predictions, references=targets) Out[23]: <pre>{'score': 15.796125110909543,\n 'counts': [128, 58, 28, 13],\n 'totals': [264, 254, 244, 235],\n 'precisions': [48.484848484848484,\n  22.834645669291337,\n  11.475409836065573,\n  5.531914893617022],\n 'bp': 0.9701515036966302,\n 'sys_len': 264,\n 'ref_len': 272}</pre> In\u00a0[74]: Copied! <pre>greedy_sequences_generator = GreedyGenerator(\n    use_tqdm=True,\n    temperature=0.9,\n    top_k_sampling=100,\n    top_p_sampling=0.8,\n    device=model.device,\n    sort_inputs_by_size=True,\n    multinomial_sampling=True,\n    generation_forward=generate,\n    batch_size=model.generation_config.batch_size,\n    max_length=model.generation_config.max_length,\n    eos_token_id=model.generation_config.eos_token_id,\n    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n)\n</pre> greedy_sequences_generator = GreedyGenerator(     use_tqdm=True,     temperature=0.9,     top_k_sampling=100,     top_p_sampling=0.8,     device=model.device,     sort_inputs_by_size=True,     multinomial_sampling=True,     generation_forward=generate,     batch_size=model.generation_config.batch_size,     max_length=model.generation_config.max_length,     eos_token_id=model.generation_config.eos_token_id,     decoder_start_token_id=model.generation_config.decoder_start_token_id, ) In\u00a0[75]: Copied! <pre>prediction_ids = greedy_sequences_generator.generate(input_texts)\npredictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\nlen(input_texts), len(predictions), len(targets)\n</pre> prediction_ids = greedy_sequences_generator.generate(input_texts) predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True) len(input_texts), len(predictions), len(targets) <pre>Generating Sequences:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> Out[75]: <pre>(10, 10, 10)</pre> In\u00a0[76]: Copied! <pre>bleu_scorer.compute(predictions=predictions, references=targets)\n</pre> bleu_scorer.compute(predictions=predictions, references=targets) Out[76]: <pre>{'score': 18.537916023808666,\n 'counts': [132, 62, 33, 19],\n 'totals': [266, 256, 246, 237],\n 'precisions': [49.62406015037594,\n  24.21875,\n  13.414634146341463,\n  8.016877637130802],\n 'bp': 0.9776961023999414,\n 'sys_len': 266,\n 'ref_len': 272}</pre> In\u00a0[27]: Copied! <pre>beam_search_sequences_generator = BeamSearchGenerator(\n    beam_width=4,\n    use_tqdm=True,\n    length_penalty=0.6,\n    device=model.device,\n    sort_inputs_by_size=True,\n    generation_forward=generate,\n    batch_size=model.generation_config.batch_size,\n    max_length=model.generation_config.max_length,\n    eos_token_id=model.generation_config.eos_token_id,\n    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n)\n</pre> beam_search_sequences_generator = BeamSearchGenerator(     beam_width=4,     use_tqdm=True,     length_penalty=0.6,     device=model.device,     sort_inputs_by_size=True,     generation_forward=generate,     batch_size=model.generation_config.batch_size,     max_length=model.generation_config.max_length,     eos_token_id=model.generation_config.eos_token_id,     decoder_start_token_id=model.generation_config.decoder_start_token_id, ) In\u00a0[28]: Copied! <pre>prediction_ids = beam_search_sequences_generator.generate(input_texts)\npredictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\nlen(input_texts), len(predictions), len(targets)\n</pre> prediction_ids = beam_search_sequences_generator.generate(input_texts) predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True) len(input_texts), len(predictions), len(targets) <pre>Generating Sequences:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> Out[28]: <pre>(10, 10, 10)</pre> In\u00a0[29]: Copied! <pre>bleu_scorer.compute(predictions=predictions, references=targets)\n</pre> bleu_scorer.compute(predictions=predictions, references=targets) Out[29]: <pre>{'score': 20.16216711910865,\n 'counts': [134, 67, 38, 21],\n 'totals': [261, 251, 241, 232],\n 'precisions': [51.34099616858238,\n  26.693227091633467,\n  15.767634854771785,\n  9.051724137931034],\n 'bp': 0.958730185172926,\n 'sys_len': 261,\n 'ref_len': 272}</pre> In\u00a0[36]: Copied! <pre>beam_search_sequences_generator = BeamSearchGenerator(\n    beam_width=4,\n    use_tqdm=True,\n    temperature=0.9,\n    top_k_sampling=100,\n    length_penalty=0.6,\n    top_p_sampling=0.8,\n    device=model.device,\n    sort_inputs_by_size=True,\n    multinomial_sampling=True,\n    generation_forward=generate,\n    batch_size=model.generation_config.batch_size,\n    max_length=model.generation_config.max_length,\n    eos_token_id=model.generation_config.eos_token_id,\n    decoder_start_token_id=model.generation_config.decoder_start_token_id,\n)\n</pre> beam_search_sequences_generator = BeamSearchGenerator(     beam_width=4,     use_tqdm=True,     temperature=0.9,     top_k_sampling=100,     length_penalty=0.6,     top_p_sampling=0.8,     device=model.device,     sort_inputs_by_size=True,     multinomial_sampling=True,     generation_forward=generate,     batch_size=model.generation_config.batch_size,     max_length=model.generation_config.max_length,     eos_token_id=model.generation_config.eos_token_id,     decoder_start_token_id=model.generation_config.decoder_start_token_id, ) In\u00a0[37]: Copied! <pre>prediction_ids = beam_search_sequences_generator.generate(input_texts)\npredictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True)\nlen(input_texts), len(predictions), len(targets)\n</pre> prediction_ids = beam_search_sequences_generator.generate(input_texts) predictions = tokenizer.batch_decode(prediction_ids, skip_special_tokens=True) len(input_texts), len(predictions), len(targets) <pre>Generating Sequences:   0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> Out[37]: <pre>(10, 10, 10)</pre> In\u00a0[38]: Copied! <pre>bleu_scorer.compute(predictions=predictions, references=targets)\n</pre> bleu_scorer.compute(predictions=predictions, references=targets) Out[38]: <pre>{'score': 21.700676010280976,\n 'counts': [134, 70, 41, 25],\n 'totals': [261, 251, 241, 232],\n 'precisions': [51.34099616858238,\n  27.888446215139442,\n  17.012448132780083,\n  10.775862068965518],\n 'bp': 0.958730185172926,\n 'sys_len': 261,\n 'ref_len': 272}</pre>"},{"location":"examples/huggingface_encoder_decoder/#encoder-decoder-architecture","title":"Encoder-Decoder Architecture\u00b6","text":""},{"location":"examples/huggingface_encoder_decoder/#load-the-model-and-dataset","title":"Load the Model and Dataset\u00b6","text":""},{"location":"examples/huggingface_encoder_decoder/#preparation-and-utility-functions","title":"Preparation and utility functions\u00b6","text":""},{"location":"examples/huggingface_encoder_decoder/#translate-with-huggingface-generate-method","title":"Translate with Huggingface <code>generate</code> method\u00b6","text":""},{"location":"examples/huggingface_encoder_decoder/#using-greedy-method","title":"Using Greedy method\u00b6","text":""},{"location":"examples/huggingface_encoder_decoder/#with-multinomial-sampling-top_k-top_p-and-temperature","title":"With multinomial sampling, top_k, top_p, and temperature\u00b6","text":""},{"location":"examples/huggingface_encoder_decoder/#using-beam-search-of-width-4","title":"Using beam search of width 4\u00b6","text":""},{"location":"examples/huggingface_encoder_decoder/#with-multinomial-top-p-top-k-sampling-and-temperature","title":"With multinomial, top-p, top-k sampling and temperature\u00b6","text":""},{"location":"examples/huggingface_encoder_decoder/#translate-using-generate-sequences","title":"Translate using generate-sequences\u00b6","text":""},{"location":"examples/huggingface_encoder_decoder/#greedy-generation","title":"Greedy Generation\u00b6","text":""},{"location":"examples/huggingface_encoder_decoder/#with-multinomial-and-top-k-top-p-sampling-and-temperature","title":"With multinomial and top-k, top-p sampling, and temperature\u00b6","text":""},{"location":"examples/huggingface_encoder_decoder/#beam-search-generation","title":"Beam Search Generation\u00b6","text":""},{"location":"examples/huggingface_encoder_decoder/#with-multinomial-top-ptop-k-sampling-and-temperature","title":"With multinomial, top-p,top-k sampling, and temperature\u00b6","text":""}]}